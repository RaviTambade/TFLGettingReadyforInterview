# DevOps Interview Questions

<hr/>
1.What are the popular DevOps tools that you use?
2.What are the main benefits of DevOps?
3.What is the typical DevOps workflow you use in your organization?
4.How do you take DevOps approach with Amazon Web Services?
5.How will you run a script automatically when a developer commits a change into GIT?
6.What are the main features of AWS OpsWorks Stacks?
7.How does CloudFormation work in AWS?
8.What is CICD in DevOps?
9.What are the best practices of Continuous Integration (CI)?
10.What are the benefits of Continuous Integration (CI)?
11.What are the options for security in Jenkins?
12. What are the main benefits of Chef?
13. What is the architecture of Chef?
14. What is a Recipe in Chef?
 15. What are the main benefits of Ansible?
 16.What are the main use cases of Ansible?
17.What is Docker Hub?
18.What is your favourite scripting language for DevOps?
19.What is Multi-factor authentication?
20.What are the main benefits of Nagios?
21.What is State Stalking in Nagios?
22.What are the main features of Nagios?
23.What is Puppet?
24.What is the architecture of Puppet?
25.What are the main use cases of Puppet Enterprise?
26.What is the use of Kubernetes?
27.What is the architecture of Kubernetes?
28.How does Kubernetes provide high availability of applications in a Cluster?
29.Why Automated Testing is a must requirement for DevOps?
30.What is Chaos Monkey in DevOps?
31.How do you perform Test Automation in DevOps?
32.What are the main services of AWS that you have used?
33.Why GIT is considered better than CVS for version control system?
34.What is the difference between a Container and a Virtual Machine?
35.What is Serverless architecture?
36.What are the main principles of DevOps? 
37.Are you more Dev or more Ops? 
38.What is a REST service?
39.What are the Three Ways of DevOps?
40.How do you apply DevOps principles to make system Secure?
41.What is Self-testing Code? 
42.What is a Deployment Pipeline? 
43.What are the main features of Docker Hub?
44.What are the security benefits of using Container based system?
45.How many heads can you create in a GIT repository?
46.What is a Passive check in Nagios?
47.What is a Docker container? 
48.How will you remove an image from Docker? 
49.What are the common use cases of Docker? 
50.Can we lose our data when a Docker Container exits? 
51.What is Docker?
52.What is the difference between Docker image and Docker container?
53.How is a Docker container different from a hypervisor? 
54.Can we write compose file in json file instead of yaml?
55.Can we run multiple apps on one server with Docker?
56.What are the main features of Docker-compose?
57.What is the most popular use of Docker?
58.What is the role of open source development in the popularity of Docker?
59.What is the difference between Docker commands: up, run and start?
60.What is Docker Swarm?
61.What are the features of Docker Swarm?
62.What is a Docker Image?
63.What is a Docker Container?
64.What is Docker Machine?
65.Why do we use Docker Machine?
66.How will you create a Container in Docker?
67.Do you think Docker is Application-centric or Machine-centric?
68.Can we run more than one process in a Docker container?
69.What are the objects created by Docker Cloud in Amazon Web Services (AWS) EC2?
70.How will you take backup of Docker container volumes in AWS S3?
71.What are the three main steps of Docker Compose?
72.What is Pluggable Storage Driver architecture in Docker based containers?
73.What are the main security concerns with Docker based containers?
74.How can we check the status of a Container in Docker?  
75.What are the main benefits of using Docker?
76.How does Docker simplify Software Development process?
77.What is the basic architecture behind Docker?
78.What are the popular tasks that you can do with Docker Command line tool?
79.What type of applications- Stateless or Stateful are more suitable for Docker Container?
80.How can Docker run on different Linux distributions?
81.Why do we use Docker on top of a virtual machine?
82.How can Docker container share resources?
83.What is the difference between Add and Copy command in a Docker file?
84.What is Docker Entry point?
85.What is ONBUILD command in Docker?
86.What is Build cache in Docker?
87.What are the most common instructions in Dockerfile?
88.What is the purpose of EXPOSE command in Dockerfile?
89.What are the different kinds of namespaces available in a Container?
108.Why do we use API in cloud computing environment?
109.What are the different areas of Security Management in cloud?  
110.What are the main cost factors of cloud based data center?
111.How can we measure the cloud-based services?
112.How a traditional datacenter is different from a cloud environment?
113.How will you optimize availability of your application in a Cloud environment?
114.What are the requirements for implementing IaaS strategy in Cloud?  
115.What is the scenario in which public cloud is preferred over private cloud?
116.Do you think Cloud Computing is a software application or a hardware service?
117.Why companies now prefer Cloud Computing architecture over Client Server Architecture?
118.What are the main characteristics of Cloud Computing architecture?  
119.How databases in Cloud computing are different from traditional databases?
120.What is Virtual Private Network (VPN)?
121.What are the main components of a VPN?
122.How will you secure the application data for transport in a cloud environment?
123.What are the large-scale databases available in Cloud? 
124.What are the options for open source NoSQL database in a Cloud environment?
125.What are the important points to consider before selecting cloud computing?
126.What is a System integrator in Cloud computing?
127.What is virtualization in cloud computing?
128.What is Eucalyptus in a cloud environment?
129.What are the main components of Eucalyptus cloud architecture?
130.What is Auto-scaling in Cloud computing?
131.What are the benefits of Utility Computing model?
132.What is a Hypervisor in Cloud Computing?
133.What are the different types of Hypervisor in Cloud Computing?
134.Why Type-1 Hypervisor has better performance than Type-2 Hypervisor?
135.What is CaaS?
136.How is Cloud computing different from computing for mobile devices?
137.Why automation of deployment is very important in Cloud architecture?
138.What are the main components in Amazon Cloud?
139.What are main components in Google Cloud?
140.What are the major offerings of Microsoft Azure Cloud?
141.What are the reasons of popularity of Cloud Computing architecture?
142.What are the Machine Learning options from Google Cloud?
143.How will you optimize the Cloud Computing environment?
144.Do you think Regulations and Legal Compliance is an important aspect of Cloud Computing?
145.How will you remove all files in current directory? Including the files that are two levels down in a sub-directory.
146.What is the difference between the –v and –x options in Bash shell scripts?
147.What is a Filter in Unix command?
148.What is Kernel in Unix operating system?
149.What is a Shell in Unix OS
150.What are the different shells in Unix that you know about?
151.What is the first character of the output in ls –l command ?
152.What is the difference between Multi-tasking and Multi-user environment?
153.What is an Inode in Unix?
154.What is the difference between absolute path and relative path in Unix file system?
155.What are the main responsibilities of a Unix Shell?
156.What is a Shell variable?
157.What are the important Shell variables that are initialized on starting a Shell?
158.How will you set the value of Environment variables in Unix?
159.What is the difference between a System Call and a library function?
160.What are the networking commands in Unix that you have used?
161.What is a Pipeline in Unix?
162.What is the use of tee command in Unix?
163.How will you count the number of lines and words in a file in Unix?
164.What is Bash shell?
165.How will you search for a name in Unix files?
166.What are the popular options of grep command in Unix?
167.What is the difference between whoami and who am i commands in Unix?
168.What is a Superuser in Unix?
169.How will you check the information about a process in Unix?
170.What is the use of more command with cat command?
171.What are the File modes in Unix?
172.We wrote a shell script in Unix but it is not doing anything. What could be the reason?
177.What is the significance of 755 in chmod 755 command?
178.How can we run a process in background in Unix? How can we kill a process running in background?
179.How will you create a read only file in Unix?
180.How does alias work in Unix?
181.How can you redirect I/O in Unix?
182.What are the main steps taken by a Unix Shell for processing a
183.What is a Sticky bit in Unix?
184.What are the different outputs from Kill command in Unix?
185.How will you customize your environment in Unix?
186.What are the popular commands for user management in Unix?
187.How will you debug a shell script in Unix?
188.What is the difference between a Zombie and Orphan process in Unix?
189.How will you check if a remote host is still alive?
190.How will you get the last executed command in Unix?
191.What is the meaning of<b> “2>&1” </b>in a Unix shell?
192.How will you find which process is taking most CPU time in Unix?
193.What is the difference between Soft link and Hard link in Unix?
194.How will you find which processes are using a file?
195.What is the purpose of nohup in Unix?
196.How will you remove blank lines from a file in Unix?
197.How will you find the remote hosts that are connecting to your system on a specific port in Unix?

<hr/>
# DevOps Interview Questionsb with Answers
<hr/>
<details>
 <summary>1.What are the popular DevOps tools that you use?</summary>
  <p>We use following tools for work in DevOps:</p>
    <ol>
        <li>Jenkins : This is an open source automation server used as a continuous integration tool. We can build,deploy and run automated tests with Jenkins.</li>
        <li>GIT:It is a version control tool used for tracking changes in files and software.</li>
        <li>Docker : This is a popular tool for containerization of services. It is very useful in Cloud based deployments.</li>
        <li>Nagios :We use Nagios for monitoring of IT infrastructure.</li>
        <li>Splunk :This is a powerful tool for log search as wel as monitoring production systems.</li>
        <li>Puppet :We use Puppet to automate our DevOps work so that it is reusable.</lI>
    </ol>
</details>

<details>
 <summary>2.What are the main benefits of DevOps?</summary>
  <p>DevOps is a very popular trend in Software Development. Some ofthe main benefits of DevOps are as follows:</p>
    <ol>
        <li> <b>Release Velocity</b> : DevOps practices help in increasing the release velocity. We can release code to production more often and with more confidence.</li>
        <li><b>Development Cycle</b> : With DevOps, the complete Development cycle from initial design to production deployment becomes shorter</li>
        <li><b>Deployment Rollback</b> : In DevOps, we plan for any failure in deployment rolback due to a bug in code or issue in production. This gives confidence in releasing feature without worrying about downtime for rolback.</li>
        <li><b>Defect Detection</b>: With DevOps approach, we can catch defects much earlier than releasing to production It improvesthe quality of the software</li>
        <li><b>Recovery from Failure</b> :In case of a failure, we can recover very fast with DevOps process.</li>
        <li><b>Collaboration</b>:With DevOps, colaboration between development and operations professionals increases.</lI>
         <li><b>Performance-oriented</b> : With DevOps, organization folows performance-oriented culture in which teams become more productive and more innovative.</lI>
    </ol>
</details>

<details>
 <summary>3.What is the typical DevOps workflow you use in your organization?</summary>
  <p>The typical DevOps workflow in our organization is as follows:</p>
    <ol>
        <li>We use Atlassian Jira for writing requirements and tracking tasks.</li>
        <li>Based on the Jira tasks, developers check in code into GIT version control system.</li>
        <li>The code checked into GIT is built by using Apache Maven.</li>
        <li>The build processis automated with Jenkins.</li>
        <li>Code built on Jenkins is sent to organization’s Artifactory.</li>
        <li>Jenkins automaticaly picks the libraries from Artifactory and deploys it to Production</li>
        <li>During Production deployment Docker images are used to deploy same code on multiple hosts.</li>
        <li>Once code is deployed to Production, we use Nagios to monitor the health of production servers.</lI>
        <li>Splunk based alerts inform us of any issues or exceptions in production.</li>
    </ol>
</details>


<details>
 <summary>4.How do you take DevOps approach with Amazon Web Services?</summary>
  <p>Amazon Web Services (AWS) provide many tools and features to deploy and manage applications in AWS. As per DevOps, we treat infrastructure as code. We mainly use folowing two services from AWS for DevOps:</p>
    <ol>
        <li><b>CloudFormation</b> : We use AWS CloudFormation to create and deploy AWS resources by using templates. We can describe our dependencies and pass special parameters in these templates.  CloudFormation can read these templates and deploy the application and resources in AWS cloud.</li>
        <li><b>OpsWorks </b>: AWS provides another service called OpsWorks that is used for configuration management by utilizing Chef framework. We can automate server configuration, deployment and management by using OpsWorks. It helps in managing EC2 instances in AWS as wel as any on-premises servers.</li>
    </ol>
</details>

<details>
 <summary>5.How will you run a script automatically when a developer commits a change into GIT?</summary>
  <p>GIT provides the feature to execute custom scripts when certain event occurs in GIT. This feature is caled hooks.</p>
  <p>We can write two types of hooks.</p>
    <ol>
        <li>Client-side hooks</li>
        <li>Server-side hooks</li>
    </ol>
    <p>For this case, we can write a Client-side post-commit hook. This hook wil execute a custom script inwhich we can add the message and code that we want to run automaticaly with each commit.</p>
</details>

<details>
 <summary>6.What are the main features of AWS OpsWorks Stacks?</summary>
  <p>Some of the main features of AWS OpsWorks Stacks are as follows:</p>
    <ol>
        <li>Server Support:AWS OpsWorks Stacks we can automate operational tasks on any server in AWS as well as our own data center.</li>
        <li>Scalable Automation : We get automated scaling support with AWS OpsWorks Stacks. Each new instance in AWS can read configuration from OpsWorks. It can even respond to system events in same way as other instances do. </li>
        <li>The code checked into GIT is built by using Apache Maven.</li>
        <li>The build processis automated with Jenkins.</li>
        <li>Code built on Jenkins is sent to organization’s Artifactory.</li>
        <li>Jenkins automaticaly picks the libraries from Artifactory and deploys it to Production</li>
        <li>During Production deployment Docker images are used to deploy same code on multiple hosts.</li>
        <li>Once code is deployed to Production, we use Nagios to monitor the health of production servers.</lI>
        <li>Splunk based alerts informus of any issues or exceptions in production.</li>
    </ol>
</details>

<details>
 <summary>7.How does CloudFormation work in AWS?</summary>
  <p>AWS CloudFormation is used for deploying AWS resources.</p>
  <p>In CloudFormation, we have to first create a template for a resource. A template is a simple text file that contains information about a stack on AWS. A stack is a collection of AWS resourced that we want to deploy together in an AWS as a group</p>
  <p> Once the template is ready and submitted to AWS, CloudFormation wil create al the resources in the template. This helps in automation of building new environments in AWS</p>
</details>

<details>
 <summary>8.What is CICD in DevOps?</summary>
  <p>CICD stands for Continuous Integration and Continuous Delivery. These are two different concepts that are complementary to each other.</p>
  <p><b>Continuous Integration (CI)</b> : In CI al the developer work is merged to main branch several times a day. This helps in reducing integration problems</p>
  <p>In CI we try to minimize the duration for which a branch remains checked out. A developer gets early feedback on the new code added to main repository by using CI.</p>   
  <p><b>Continuous Delivery (CD) </b>: In CD, a software team plans to deliver software in short cycles. They perform development,testing and release in such a short time that incremental changes can be easily delivered to production</p>
  <p>In CD, as a DevOps we create a repeatable deployment process that can help achieve the objective of Continuous Delivery.</p>
</details>

<details>
 <summary>9.What are the best practices of Continuous Integration (CI)?</summary>
  <p>Some of the best practices of Continuous Integration (CI) are as follows:</p>
    <ol>
        <li><b>Build Automation</b> : In CI, we create such a build environment that even with one command build can be triggered. This automation is done all the way up to deployment to Production environment.</li>
        <li><b>Main Code Repository</b> : In CI, we maintain a main branch in code repository that stores all the Production ready code. This is the branch that we can deploy to Production any time.</li>
        <li><b>Self-testing build</b> : Every build in CI should be self-tested. It means with every build there is a set of tests that runs to ensure that changes are of high quality.</li>
        <li> <b>Every day commits to baseline </b>: Developers will commit all of theirs changes to baseline every day. This ensures that there is no big pile up of code waiting for integration with the main repository for a long time.</li>
        <li><b>Build every commit to baseline</b> : With Automated Continuous Integration, every time a commit is made into baseline, a build is triggered. This helps in confirming that every change integrates correctly.</li>
        <li><b>Fast Build Process</b> : One of the requirements of CI is to keep the build process fast so that we can quickly identify any problem.</li>
        <li><b>Production like environment testing </b>:In CI, we maintain a production like environment also known as preproduction or staging environment, which is very close to Production  environment. We perform testing in this environment to check for any integration issues.</li>
        <li> <b>Publish Build Results</b> : We publish build results on a common site so that everyone can see these and take corrective actions.</lI>
        <li><b>Deployment Automation</b>:The deployment process is automated to the extent that in a build process we can add the step of deploying the code to a test environment. On this test environment all the stakeholders can access and test the latest deliver</li>
    </ol>
</details>

<details>
 <summary>10.What are the benefits of Continuous Integration (CI)?</summary>
  <p>The benefits of Continuous Integration (CI) are as follows:</p>
    <ol>
        <li>CI makes the current build constantly available for testing, demo and release purpose</li>
        <li>With CI, developers write modular code that works wel with frequent code check-ins</li>
        <li>Incase of a unit test failure or bug, developer can easily revert back to the bug-free state ofthe code</li>
        <li>There is drastic reduction in chaos on release day with CI practices.</li>
        <li>With CI, we can detect Integration issues much earlier in the process.</li>
        <li>Automated testing is one very useful sideefect of implementing CI.</li>
        <li>Al the stakeholders including business partners can see the smal changes deployed into pre-production environment. This provides early feedback on the changes to software.</li>
        <li>Automated CI and testing generates metrics like code-coverage, code complexity that help in improving the development process.</li>
    </ol>
</details>

<details>
 <summary>11.What are the options for security in Jenkins?</summary>
  <p>In Jenkins, it is very important to make the system secure by setting user authentication and authorization. To do this we have to do following:</p>
    <ol>
        <li>First we have to set up the Security Realm. We can integrate Jenkins with LDAP server to create user authentication.</li>
        <li>Second part is to set the authorization for users. This determines which user has access to what resources.</li> 
    </ol>
  <p>In Jenkins some of the options to setup security are as follows:</p>
    <ol>
        <li> We canuse Jenkins’ own User Database.</li>
        <li>We can use LDAP plugin to integrate Jenkins with LDAP server.</li>
        <li>We can also setup Matrix based security on Jenkins.</li>
    </ol>
</details>

<details>
 <summary>12. What are the main benefits of Chef?</summary>
  <p>Chef is an automation tool for keeping infrastructure as code. It has many benefits. Some of these are as follows:</p>
    <ol>
        <li>CloudDeployment:We can use Chef to perform automated deployment in Cloud environment.</li>
        <li>Multi-cloudsupport:With Chef we can even use multiple cloud providers for our infrastructure.</li>
        <li>HybridDeployment :Chef supports both Cloud based as well as data center-based infrastructure.</li>
        <li>High Availability : With Chef automation, we can create high availability environment. In case of hardware failure, Chef can maintain or start new servers in automated way to maintain highly available environment.</li>
    </ol>
</details>

<details>
 <summary>13. What is the architecture of Chef?</summary>
  <p>Chef is composed of many components like Chef Server, Client etc. Some of the main components in Chef are as follows:</p>
    <ol>
        <li>Client: These are the nodes or individual users that communicate with Chef server.</li>
        <li>ChefManage : This is the web console that is used for interacting with Chef Serve</li>
        <li>Load Balancer : All the Chef server API requests are routed through Load Balancer. It is implemented in Nginx</li>
        <li>Bookshelf: This i sthe component that stores cookbooks. All the cookbooks are stored in a repository. It is separate storage from the Chef server.</li>
        <li>PostgreSQL:This is the data repository for Chef server.</li>
        <li>Chef Server : This is the hub for configuration data. Al the cookbooks and policies are stored in it. It can scale to the size of any enterprise. </li>
    </ol>
</details>

<details>
 <summary>14. What is a Recipe in Chef?</summary>
    <p>In any organization, Recipe is the most fundamental configuration element. It is written in Ruby language. It is a collection of resources defined by using patterns.</p>
    <p>A Recipe isstored ina Cookbook and it may have dependency on other Recipe.</p>
    <p>We can tag Recipe to create some kind of grouping.</p>
    <p>We have to add a Recipe in run-list before using it by chef-client.</p>
    <p>It always maintains the execution order specified in run-list</p>
 </details>


<details>
 <summary> 15. What are the main benefits of Ansible?</summary>
  <p>Ansible is a powerful tool for IT Automation for large scale and complex deployments. It increases the productivity of team.</p>
  <p>Some of the main benefits of Ansible are as folows:</p>
    <ol>
        <li>Productivity :It helps in delivering and deploying with speed. It increases productivity in an organization.</li>
        <li>Automation : Ansible provides very good options for automation. With automation, people can focus on delivering smart solutions.</li>
        <li> Large-scale :Ansible can be used in small as well as very large-scale organizations.</li>
        <li>Simple DevOps : With Ansible, we can write automation in a human-readable language. This simplifies the task of DevOps.</li>
    </ol>
</details>

<details>
 <summary> 16.What are the main use cases of Ansible?</summary>
  <p>Some of the popular use cases of Ansible are as follows:</p>
    <ol>
        <li><b>AppDeployment</b> :With Ansible, we can deploy appsin a reliable and repeatable way.</li>
        <li><b>A ConfigurationManagement</b> :Ansible supports the automation of configuration management across multiple environments.</li>
        <li><b>Continuous Delivery</b> :We can release updates with zero downtime with Ansible</li>
        <li><b>Security </b>:We can implement complex security policies with Ansible</li>
        <li><b>Compliance </b>: Ansible helps in verifying and organization’s systems in comparison with the rules and regulations</li>
        <li><b>Provisioning</b> :We can provide newsystems and resources to other users with Ansible.</li>
        <li><b>Orchestration</b>:Ansible can be used in orchestration of complex deployment in a simple way</li>
    </ol>
</details>


<details>
 <summary>17.What is Docker Hub?</summary>
  <p>Docker Hub is a cloud-based registry. We can use Docker Hub to link code repositories. We can even build images and store them in Docker Hub. It also provides links to Docker Cloud to deploy the images to our hosts</p>
  <p>Docker Hub is a central repository for container image discovery, distribution, change management, workflow automation and team collaboration.</p>
</details>

<details>
 <summary>18.What is your favourite scripting language for DevOps?</summary>
  <p>In DevOps, we use diferent scripting languages for diferent purposes. There is no single language that can work in all the scenarios. Some of the popular scripting languages that we use are as folows:</p>
    <ol>
        <li><b>Bash :</b>On Unix based systems we use Bash shel scripting for automating tasks.</li>
        <li><b>Python :</b> For complicated programming and large modules we use Python. We can easily use a wide variety of standard libraries with Python.</li>
        <li><b>Groovy :</b> This is a Java based scripting language. We need JVM instaled in an environment to use Groovy. It is very powerful and it provides very powerful features.</li>
        <li><b>Perl :</b> This is another language that is very useful for text parsing. We use it in web applications.</li>
    </ol>
</details>

<details>
 <summary>19.What is Multi-factor authentication?</summary>
  <p>In security implementation, we use Multi-factor authentication (MFA). In MFA, a user is authenticated by multiple means before giving access to a resource or service. It is different from simple user/  password based authentication.</p>
  <p>The most popular implementation of MFA is Two-factor authentication. In most of the organizations, we use username/password and an RSA token as two factors for authentication.</p>
  <p>With MFA, the system becomes more secure and it cannot be easily hacked.</p>   
</details>


<details>
 <summary>20.What are the main benefits of Nagios?</summary>
  <p>Nagios is open source software to monitor systems, networks and infrastructure. The main benefits of Nagios are as folows:</p>
    <ol>
        <li><b>Monitor :</b>DevOps can configure Nagios to monitor IT infrastructure components, system metrics and network protocols.</li>
        <li><b>Alert :</b> Nagios will send alerts when a critical component in infrastructure fails.</li>
        <li><b> Response :</b> DevOps acknowledges alerts and takes corrective actions.</li>
        <li><b>Report :</b> Periodicaly Nagios can publish/send reports on outages, events and SLAs etc.</li>
        <li><b>Maintenance :</b>During maintenance windows, we can also disable alerts.</li>
        <li><b>Planning :</b>Based on past data, Nagios helps in infrastructure planning and upgrades.</li>
    </ol>
</details>


<details>
 <summary>21.What is State Stalking in Nagios?</summary>
  <p>State Stalking is a very useful feature. Though all the users do not use it all the time, it is very helpful when we want to investigate an issue</p>
  <p>In State Stalking, we can enable stalking on a host. Nagios will monitor the state of the host very carefuly and it will log any changes in the state.</p>
  <p>By this we can identify what changes might be causing an issue on the host.</p>   
</details>

<details>
 <summary>22.What are the main features of Nagios?</summary>
  <p>Some of the main features of Nagios are as folows:</p>
    <ol>
        <li><b>Visibility :</b>Nagios provides a centralized view of the entire IT infrastructure.</li>
        <li><b>Monitoring :</b>We can monitor all the mission critical infrastructure components with Nagios.</li>
        <li><b>Proactive Planning :</b>With Capacity Planning and Trending we can proactively plan to scale up or scale down the infrastructure.</li>
        <li><b>Extendable :</b>Nagios is extendable to a third party tools in APIs.</li>
        <li><b>Multi-tenant :</b>Nagios supports multi-tenants architecture.</li>
    </ol>
</details>


<details>
 <summary>23.What is Puppet?</summary>
  <p>Puppet Enterprise is a DevOps software platform that is used for automation of infrastructure operations. It runs on Unix as well as on Windows.</p>
  <p>We can define system configuration by using Puppet’s language or RubyDSL</p>
  <p>The system configuration described in Puppet’s language can be distributed to a target system by using RESTAPI calls.</p>   
</details>


<details>
 <summary>24.What is the architecture of Puppet?</summary>
  <p>Puppet is Open Source software. It is based on Client-server architecture. It is a Model Driven system. The client is also called Agent. And server is called Master.</p>
  <p>It has folowing architectural components:</p>
    <ol>
        <li><b>Configuration Language :</b> Puppet provides a language that is used to configure Resources. We have to specify what Action has to be applied to which Resource.</br>The Action has three items for each Resource: type, title and list of attributes of a resource. Puppet code is written in Manifests files.</li>
        <li><b>Resource Abstraction :</b>We can create Resource Abstraction in Puppet so that we can configure resources on different platforms. Puppet agent uses a Facter for passing the information of an environment to Puppet server. In Facter we have information about IP, hostname, OS etc of the environment</li>
        <li><b>Transaction :</b>In Puppet, Agent sends Facter to Master server. Master sends back the catalog to Client. Agent applies any configuration changes to system. Once all changes are applied, the result is sent to Server</li>
    </ol>
</details>


<details>
 <summary>25.What are the main use cases of Puppet Enterprise?</summary>
  <p>We can use Puppet Enterprise for folowing scenarios:</p>
    <ol>
        <li><b>Node Management :</b>We can manage a large number of nodes with Puppet.</li>
        <li><b>Code Management :</b>With Puppet we can define Infrastructure as code. We can review, deploy, and test the environment configuration for Development, Testing and Production environments.</li>
        <li><b>Reporting & Visualization :</b>: Puppet provides Graphical tools to visualize and see the exact status of infrastructure configuration.</li>
        <li><b>Provisioning Automation :</b>With Puppet we can automate deployment and creation of new servers and resources. So users and business can get their infrastructure requirements completed very fast with Puppet</li>
        <li><b>Orchestration :</b>For a large Cluster of nodes, we can orchestrate the complete process by using Puppet. It can follow the order in which we want to deploy the infrastructure environments.</li>
        <li><b>Automation of Configuration :</b>With Configuration automation, the chances of manual errors are reduced. The process becomes more reliable with this.</li>
    </ol>
</details>


<details>
 <summary>26.What is the use of Kubernetes?</summary>
  <p>We use Kubernetes for automation of large-scale deployment of Containerized applications.</p>
  <p>It is an open source system based on concepts similar to Google’s deployment process of milions of containers.</p>
  <p>It can be used on cloud, on-premise datacenter and hybrid infrastructure.</p>  
  <p>In Kubernetes we can create a cluster of servers that are connected to work as a single unit. We can deploy a containerized application to all the servers in a cluster without specifying the machine name.</p>
  <p>We have to package applications in such a way that they do not depend on a specific host.</p> 
</details>


<details>
 <summary>27.What is the architecture of Kubernetes?</summary>
  <p>The architecture of Kubernetes consists of following components:</p>
    <ol>
        <li><b>Master :</b> There is a master node that is responsible for managing the cluster. Master performs folowing functions in a cluster.</li>
              <ol>
              <li>Scheduling Applications</li>
              <li>Maintaining desired state of applications </li>
              <li>Scaling applications </li>
              <li>Applying updates to applications</li>
              </ol>
        <li><b>Nodes :</b>A Node in Kubernetes is responsible for running an application. The Node can be a Virtual Machine or a Computer in the cluster. There is software called Kubelet on each node.This software is used for managing the node and communicating with the Master node in cluster.</li>
  <p>There is a Kubernetes API that is used by Nodes to communicate with the Master. When we deploy an application on Kubernetes, we request Master to start application containers on Nodes.</p>
    </ol>
</details>


<details>
 <summary>28.How does Kubernetes provide high availability of applications in a Cluster?</summary>
  <p>In a Kubernetes cluster, there is a Deployment Controler. This controler monitors the instances created by Kubernetes in a cluster. Once a node or the machine hosting the node goes down, Deployment Controler will replace the node.</p>
  <p>It is a self-healing mechanism in Kubernetes to provide high availability of applications.</p>
  <p>Therefore in Kubernetes cluster, Kubernetes Deployment Controler is responsible for starting the instances as well as replacing the instances in case of a failure.</p>  
</details>


<details>
 <summary>29.Why Automated Testing is a must requirement for DevOps?</summary>
  <p>In DevOps approach we release software with high frequency to production. We have to run tests to gain confidence on the quality of software deliverables.</p>
  <p>Running tests manualy is a time taking process. Therefore, we first prepare automation tests and then deliver software. This ensures that we catch any defects early in our process.</p>
</details>


<details>
 <summary>30.What is Chaos Monkey in DevOps?</summary>
  <p>Chaos Monkey is a concept made popular by Netflix. In Chaos Monkey, we intentionaly try to shut down the services or create failures. By failing one or more services, we test the reliability and recovery mechanism of the Production architecture.</p>
  <p>It checks whether our applications and deployment have survival strategy built into it or not.</p>
</details>


<details>
 <summary>31.How do you perform Test Automation in DevOps?</summary>
  <p>We use Jenkins to create automated flows to run Automation tests. The first part of test automation is to develop test strategy and test cases. Once automation test cases are ready for an application, we have to plug these into each Build run.In each Build we run Unit tests, Integration tests and Functional tests.</p>
  <p>With a Jenkins job, we can automate all these tasks. Once all the automated tests pass, we consider the build as green. This helps in deployment and release processes to build confidence on the application software.</p>
</details>


<details>
 <summary>32.What are the main services of AWS that you have used?</summary>
  <p>We use following main services of AWS in our environment:</p>
    <ol>
        <li><b>EC2 :</b>This is the Elastic Compute Cloud by Amazon. It is used to for providing computing capability to a system. We can use it in places of our standalone servers. We can deploy different kinds of applications on C2.</li>
        <li><b>S3 :</b>We use S3 in Amazon for our storage needs</li>
        <li><b>DynamoDB :</b>We use DynamoDB in AWS for storing data in NoSQL database form.</li>
        <li><b>Amazon Cloud Watch :</b>We use Cloud Watch to monitor our application in Cloud.</li>
        <li><b>Amazon SNS :</b>We use Simple Notification Service to inform users about any issues in Production environment.</li> 
    </ol>
</details>


<details>
 <summary>33.Why GIT is considered better than CVS for version control system?</summary>
  <p>GIT is a distributed system. In GIT, any person can create its own branch and start checking in the code. Once the code is tested, it is merged into main GIT repo. IN between, Dev, QA and product can validate the implementation of that code.</p>
  <p>In CVS, there is a centralized system that maintains all the commits and changes. GIT is open source software and there are plenty of extensions in GIT for use by our teams.</p>
</details>

<details>
 <summary>34.What is the difference between a Container and a Virtual Machine?</summary>
  <p>We need to select an Operating System (OS) to get a specific Virtual Machine (VM). VM provides full OS to an application for running in a virtualized environment.</p>
  <p>A Container uses APIs of an Operating System (OS) to provide runtime environment to an application</p>
   <p>A Container is very light weight in comparison witha VM.</p>
   <p>VM provides higher level of security compared to a Container.</p>
   <p>A Container just provides the APIs that are required by the application.</p>
</details>

<details>
 <summary>35.What is Serverless architecture?</summary>
    <p>Serverless Architecture is a term that refers to following:</p>
      <ol>
        <li> An Application that depends on a third-party service.</li>
        <li>An Application in which code is run on ephemeral containers.</li>
      </ol>
     <p>In AWS, Lambda is a popular service to implement Serverless architecture.</p>
     <p>Another concept in Serverless Architecture is to treat code as a service or Function as a Service (FAAS).
        We just write code that can be run on any environment or server without the need of specifying which server should be used to run this code.</p>
</details>

<details>
 <summary>36.What are the main principles of DevOps? </summary>
  <p>DevOps is different from Technical Operations. It has following main principles:</p>
    <ol>
        <li> <b>Incremental</b>:  In DevOps we aim to incrementaly release software to production. We do release 
         to production more often than Waterfall approach of one large release.</li>
        <li><b>Automated</b>: To enable use to make releases more often, we automate the operations from Code Check in to deployment in Production. </li>
        <li><b>Collaborative</b> : DevOps is not only responsibility of Operations team. It is a collaborative effort of Dev, QA,Release and DevOps teams. </li>
        <li><b>Iterative</b>:  DevOps is based on Iterative principle of using a process that is repeatable. But with each iteration we aim to make the process more ef ificient and better. </li>
        <li><b>Self-Service</b>: In DevOps, we automate things and give self-service options to other teams so that they are empowered to deliver the work in their domain. </li>
    </ol>
</details>

<details>
 <summary>37.Are you more Dev or more Ops? </summary>
  <p>This is a tricky question. DevOps is a new concept and in any organization the maturity of DevOps varies from highly Operations oriented to highly DevOps oriented. In some projects teams are very mature and practice DevOps in it true form.In some projects, teams rely more on Operations team.</p>
  <p>As a DevOps person I give first priority to the needs of an organization and project. At some times I may have to perform a lot of operations work. But with each iteration, I aim to bring DevOps changes incrementally to an organization.</p>
  <p>Over time, organization/project starts seeing results of DevOps practices and embraces it fully.</p>
</details>

<details>
 <summary>38.What is a REST service?</summary>
  <p>REST is also known as Representational State Transfer. A REST service is a simple software functionality that is available over HTTP protocol. It is a lightweight service that is widely available due to the popularity of HTTP protocol. </p>
  <p>Since REST is lightweight; it has very good performance in a software system. It is also one of the foundations for creating highly scalable systems that provide a service to large number of clients.</p> 
  <p>Another key feature of a REST service is that as long as the interface is kept same, we can change the underlying implementation. E.g. Clients of REST service can keep calling the same service while we change the implementation from php to Java.</p>
</details>

<details>
 <summary>39.What are the Three Ways of DevOps?</summary>
  <p>Three Ways of DevOps refers to three basic principles of DevOps culture. These are as follows:</p>
    <ol>
        <li><b>The First Way: Systems Thinking</b> : : In this principle we see the DevOps as a flow of work from left to right. This is the time taken from Code check in to the feature being released to End customer. In DevOps culture we try to identify the bottlenecks in this.</li>
        <li><b>The Second Way: Feedback Loops</b>: Whenever there is an issue in production it is a feedback about the whole development and deployment process.We try to make the feedback loop more efficient so that teams can get the feedback much faster. It is a way of catching defect much earlier in process than it being reported by customer.</li>
        <li><b>The Third Way: Continuous Learning</b>:: We make use of first and second way principles to keep on making improvements in the overall process.This is the third principle in which over the time we make the process and our operations highly efficient, automated and error free by continuously improving them </li>
    </ol>
</details>

<details>
 <summary>40.How do you apply DevOps principles to make system Secure?</summary>
  <p>Security of a system is one of the most important goals for an organization. We use following ways to apply DevOps to security.</p>
    <ol>
        <li><b>Automated Security Testing</b>:We automate and integrate Security testing techniques for Software 
        Penetration testing and Fuzz testing in software development process.</li>
        <li><b>Early Security Checks </b>: We ensure that teams know about the security concerns at the beginning of a project, rather than at the end of delivery. It is achieved by conducting Security trainings and knowledge sharing sessions.</li>
        <li><b>Standard Process </b>: At DevOps we try to follow standard deployment and development process that has already gone through security audits. This helps in minimizing the introduction of any new security loopholes due to change in the standard process. </li>
    </ol>
</details>

<details>
 <summary>41.What is Self-testing Code? </summary>
  <p>Self-testing Code is an important feature of DevOps culture. In DevOps culture, development team members are expected to write self-testing code. It means we have to write code along with the tests that can test this code.Once the test passes, we feel confident to release the code. </p>
  <p>If we get an issue in production, we first write an automation test to validate that the issue happens in current release. Once the issue in release code is fixed, we run the same test to validate that the defect is not there. With each release we keep running these tests so that the issue does not appear anymore. </p>
  <p>One of the techniques of writing Self-testing code is Test Driven Development (TDD).</p> 
</details>

<details>
 <summary>42.What is a Deployment Pipeline? </summary>
 <p>A Deployment Pipeline is an important concept in Continuous Delivery. In Deployment Pipeline we break the build process into distinct stages. In each stage we get the feedback to move onto the next stage.</p> 
 <p>It is a collaborative effort between various groups involved in delivering software development. Often the first stage in Deployment Pipeline is compiling the code and converting into binaries.</p> 
 <p>After that we run the automated tests. Depending on the scenario, there are stages like performance testing, security check, usability testing etc in a Deployment Pipeline. </p>
 <p>In DevOps, our aim is to automate all the stages of Deployment Pipeline. With a smooth running Deployment Pipeline, we can achieve the goal of Continuous Delivery.<p>
</details>

<details>
 <summary>43.What are the main features of Docker Hub?</summary>
  <p>Docker Hub provides following main features:</p>
    <ol>
     <li><b>Image Repositories</b>: In Docker Hub we can push, pull, find and manage Docker Images. It is a big library that has images from community, of icial 
     as well as private sources. </li>
     <li><b> Automated Builds </b>: We can use Docker Hub to create new images by making changes to source code repository of the image.</li> 
     <li><b>Webhooks </b>: With Webhooks in Docker Hub we can trigger actions that can create and build new images by pushing a change to repository. </li>
     <li><b>Github/Bitbucket integration </b>: Docker Hub also provides integration with Github and Bitbucket systems.</li>
    </ol>
</details>

<details>
<summary>44.What are the security benefits of using Container based system?</summary>
 <p>Some of the main security benefits of using a Container based system are as follows:</p>
   <ol>
    <li><b>Segregation </b>: In a Container based system we segregate the applications on dif erent containers. Each application may be running on same host but 
     in a separate container. Each application has access to ports, files and other resources that are provided to it by the container.</li>
    <li><b>Transient </b>: In a Container based system, each application is considered as a transient system. It is better than a static system that has fixed 
     environment which can be exposed overtime.</li>
    <li><b>Control </b>: We use repeatable scripts to create the containers. This provides us tight control over the software application that we want to deploy 
     and run. It also reduces the risk of unwanted changes in setup that can cause security loopholes.</li> 
    <li><b>Security Patch </b>: In a Container based system; we can deploy security patches on multiple containers in a uniform way. Also it is easier to patch a 
    Container with an application update.</li>
   </ol>
   </details>

<details>
<summary>45.How many heads can you create in a GIT repository?</summary>
 <p>There can be any number of heads in a GIT repository.</p> 
 <p>By default there is one head known as HEAD in each repository in GIT</p> 
</details>

<details>
<summary>46.What is a Passive check in Nagios?</summary>
 <p>In Nagios, we can monitor hosts and services by active checks. In addition, Nagios also supports Passive checks that are initiated by external applications.</p>
 <p>The results of Passive checks are submitted to Nagios. There are two main use cases of Passive checks:<p>
 <ol>
   <li> We use Passive checks to monitor asynchronous services that do not give positive result with Active checks at regular intervals of time.</li> 
   <li> We can use Passive checks to monitor services or applications that are located behind a firewal.</li>
 </ol>
   </details>

   <details>
<summary>47.What is a Docker container? </summary>
 <p>A Docker Container is a lightweight system that can be run on a Linux operating system or a virtual machine. It is a package of an application and related dependencies that can be run independently.</p> 
 <p>Since Docker Container is very lightweight, multiple containers can be run simultaneously on a single server or virtual machine.</p> 
 <p>With a Docker Container we can create an isolated system with restricted services and processes. A Container has private view of the operating system. It has its own process ID space, file system, and network interface.</p> 
 <p>Multiple Docker Containers can share same Kernel.</p> 
   </details>

   <details>
<summary>48. How will you remove an image from Docker? </summary>
 <p>We can use docker rmi command to delete an image from our local system. 
    Exact command is:</p> 
 <p>% docker rmi <Image Id></p>  
 <p>If we want to find IDs of al the Docker images in our local system, we can user docker images command.</p>  
 <p>% docker images</p>
 <p>If we want to remove a docker container then we use docker rm command.</p> 
 <p>% docker rm<Container Id> </p>
   </details>

   <details>
<summary>49.What are the common use cases of Docker? </summary>
   <p>Some of the common use cases of Docker are as follows:</p> 
     <ol>
      <li><b> Setting up Development Environment</b> : We can use Docker to set the development environment with the applications on which our code is dependent. 
      </li> 
      <li><b> Testing Automation Setup </b>: Docker can also help in creating the Testing Automation setup. We can setup different services and apps with Docker 
       to create the automation-testing environment. </li>
      <li><b> Production Deployment</b> : Docker also helps in implementing the Production deployment for an application.We can use it to create the exact 
       environment and process that wil be used for doing the production deployment. </li>
      </ol>
   </details>

   <details>
<summary>50. Can we lose our data when a Docker Container exits? </summary>
 <p>A Docker Container has its own file-system. In an application running on Docker Container we can write to this file-system.When the container exits, data written to file-system stil remains. When we restart the container, same data can be accessed again. </p>
 <p>Only when we delete the container, related data wil be deleted.</p>
   </details>

<details>
<summary>51. What is Docker?</summary>
  <p>Docker is Open Source software. It provides the automation of Linux application deployment in a software container. </p>
  <p>We can do operating system level virtualization on Linux with Docker.</p> 
  <p>Docker can package software in a complete file system that contains software code, runtime environment, system  tools, & libraries that are required to install and run the software on a server.</p> 
   </details>

<details>
<summary>52. What is the difference between Docker image and Docker container?</summary>
  <p>Docker container is simply an instance of Docker image.</p>
  <p>A Docker image is an immutable file, which is a snapshot of container. We create an image with build command. 
     When we use run command, an Image wil produce a container.</p> 
  <p>In programming language, an Image is a Class and a Container is an instance of the class.</p>
   </details>

<details>
<summary>53.How is a Docker container different from a hypervisor? </summary>
  <p>In a Hypervisor environment we first create a Virtual Machine and then install an Operating System on it. After that we deploy the application. The virtual machine may also be installed on different hardware configurations.</p> 
  <p>In a Docker environment, we just deploy the application in Docker. There is no OS layer in this environment. We specify libraries, and rest of the kernel is provided by Docker engine.</p>
  <p>In a way, Docker container and hypervisor are complementary to each other.</p>
   </details>

<details>
 <summary>54.Can we write compose file in json file instead of yaml?</summary>
  <p>Yes. Yaml format is a superset of json format. Therefore any json file is also a valid Yaml file</p>
  <p>If we use a json file then we have to specify in docker command that we are using a json file as follows:</p>
  <p>% docker-compose -f docker-compose.json up</p>
</details>

<details>
 <summary>55.Can we run multiple apps on one server with Docker?</summary>
  <p>Yes, theoretically we can run multiples apps on one Docker server. But in practice, it is better to run different components on separate containers</p>
  <p>With this we get cleaner environment and it can be used for multiple uses.</p>
</details>

<details>
 <summary>56.What are the main features of Docker-compose?</summary>
  <p>Some of the main features of Docker-compose are as follows</p>
    <ol>
        <li><b>Multiple environments on same Host</b>: We can use it to create multiple environments on the same host server.</li>
        <li><b>Preserve Volume Data on Container Creation</b>: Docker compose also preserves the volume data when we create a container.</li>
        <li><b>Recreate the changed Containers</b>: We can also use compose to recreate the changed containers.</li>
        <li><b>Variables in Compose file</b>: Docker compose also supports variables in compose file. In this way we can create variations of our containers.</li>
    </ol>
</details>



<details>
 <summary>57.What is the most popular use of Docker?</summary>
  <p>The most popular use of Docker is in build pipeline. With the use of Docker it is much easier to automate the development to deployment process in build pipeline.</p>
<p>We use Docker for the complete build flow from development work, test run and deployment to production environment.</p>
</details>

<details>
 <summary>58.What is the role of open source development in the popularity of Docker?</summary>
  <p>Since Linux was an open source operating system, it opened new opportunities for developers who want to contribute to open source systems.</p>
  <p>One of the very good outcomes of open source software is Docker. It has very powerful features.</p>
  <p>Docker has wide acceptance due to its usability as well as its open source approach of integrating with different systems.</p>
</details>

<details>
 <summary>59.What is the difference between Docker commands: up, run and start?</summary>
  <p>We have up and start commands in docker-compose. The run command is in docker.</p>
  <li><b>Up</b>: We use this command to build, create, start or restart all the services in a docker-compose.yml file. It also attaches
  to containers for a service. This command can also start linked services</li>
  <li><b>Run</b>: We use this command for adhoc requests. It just starts the service that we specifically want to start. We generally
  use it run specific tests or any administrative tasks.</li>
  <li><b>Start</b>: This command is used to start the container that were previously created but are not currently running. This command does not create new containers.</li>
</details>

<details>
 <summary>60.What is Docker Swarm?</summary>
  <p>Docker Swarm is used to create a cluster environment. It can turn a group of Docker engines into a Single virtual Docker
Engine. This creates a system with pooled resources. We can use Docker Swarmto scale our application.</p>
</details>

<details>
 <summary>61. What are the features of Docker Swarm?</summary>
  <p>Some of the key features of Docker Swarmare as follows:</p>
    <ol>
        <li><b>Compatible</b>: Docker Swarmis compatible with standard Docker API.</li>
        <li><b>High Scalability</b>: Swarm can scale up to as much as 1000 nodes and 50000 containers. There is almost no performance degradation at this scale in Docker Swarm.</li>
        <li><b>Networking</b>: Swarm comes with support for Docker Networking.</li>
        <li><b>High Availability</b>: We can create a highly available system with Docker Swarm. It allows use to create multiple master nodes so that in case of a failure, another node cantake over.</li>
        <li><b>Node Discovery</b>: In Docker Swarm, we can add more nodes and the new nodes can be found with any discovery service like etcd or zookeeper etc.</li>
    </ol>
</details>

<details>
 <summary>62.What is a Docker Image?</summary>
  <p>Docker Image is the blue print that is used to create a Docker Container. Whenever we want to run a container we have to
specify the image that we want to run.</p>
  <p>There are many Docker images available online for standard software. We can use these images directly from the source.</p>
  <p>The standard set of Docker Images is stored in Docker Hub Registry. We can download these from this location and use it in our environment</p>
  <p>We can also create our own Docker Image with the software that we want to run as a container.</p>

</details>

<details>
 <summary>63.What is a Docker Container?</summary>
  <p>A Docker Container is a lightweight system that can be run on a Linux operating system or a virtual machine. It is a package of
 an application and related dependencies that can be run independently.</p>
 <p>Since Docker Container is very light weight, multiple containers can be run simultaneously on a single server or virtual machine.</p>
 <p>With a Docker Container we can create an isolated system with restricted services and processes. A Container has private view of the operating system. It has its own process ID space, file system, and network interface.</p>
 <p>Multiple Docker Containers can share same Kernel.</p>
</details>


<details>
 <summary>64.What is Docker Machine?</summary>
  <p>We can use Docker Machine to install Docker Engine on virtual hosts. It also provides commands to manage virtual hosts.</p>
 <p>Some of the popular Docker machine commands enable us to start, stop, inspect and restart a managed host.</p>
 <p>Docker Machine provides a Command Line Interface (CLI), which is very useful in managing multiple hosts.</p>
</details>

<details>
 <summary>65.Why do we use Docker Machine?</summary>
 <p>There are two main uses of Docker Machine:</p>
        <li><b>Old Desktop</b>: : If we have an old desktop and we want to run Docker then we use Docker Machine to run Docker. It is like installing a virtual machine on an old hardware system to run Docker engine.</li>
        <li><b>Remote Hosts</b>: Docker Machine is also used to provision Docker hosts on remote systems. By using Docker Machine you can install Docker Engine on remote hosts and configure clients on them.</li>
</details>

<details>
 <summary>66.How will you create a Container in Docker?</summary>
  <p>To create a Container in Docker we have to create a Docker Image. We can also use an existing Image from Docker Hub Registry.</p>
 <p>We can run an Image to create the container.</p>
</details>

<details>
 <summary>67.Do you think Docker is Application-centric or Machine-centric?</summary>
  <p>Docker is an Application-centric solution. It is optimized for deployment of an application. It does not replace a machine by creating a virtual machine. Rather, it focuses on providing ease of use features to run an application.</p>
</details>


<details>
 <summary>68.Can we run more than one process in a Docker container?</summary>
  <p>Yes, a Docker Container can provide process management that can be used to run multiple processes. There are process
supervisors like runit, s6, daemontools etc that can be used to fork additional processes in a Docker container.</p>
</details>


<details>
 <summary>69.What are the objects created by Docker Cloud in Amazon Web Services (AWS) EC2?</summary>
  <p>Docker Cloud creates following objects in AWS EC2 instance:</p>
  <ol>
    <li><b>VPC</b>: :Docker Cloud creates a Virtual Private Cloud with the tag name dc-vpc. It also creates Class Less Inter-DomainRouting(CIDR) with the range of 10.78.0.0/16 .</li>
    <li><b>Subnet</b>: Docker Cloud creates a subnet in each Availability Zone (AZ). In Docker Cloud, each subnet is tagged with dc-subnet.</li>
    <li><b>Internet Gateway</b>:  Docker Cloud also creates an internet gateway with name dc-gateway and attaches it
    to the VPC created earlier.</li>
    <li><b>Routing Table</b>: Docker Cloud also creates a routing table named dc-route-table in Virtual Private Cloud. In this Routing Table Docker Cloud associates the subnet with the Internet Gateway.</li>
    </ol>
</details>


<details>
 <summary>70.How will you take backup of Docker container volumes in AWS S3?</summary>
  <p>We can use a utility named Dockup provided by Docker Cloud to take backup of Docker container volumes in S3</p>
</details>


<details>
 <summary>71.What are the three main steps of Docker Compose?</summary>
  <p>Three main steps of Docker Compose are as follows:</p>
    <ol>
    <li><b>Environment</b>: We first define the environment of our application with a Dockerfile. It can be used to recreate
    the environment at a later point of time.</li>
    <li><b>Services</b>: Then we define the services that make our app in docker-compose.yml. By using this file we
    can define how these services can be run together in an environment.</li>
    <li><b>Run</b>: The last step is to run the Docker Container. We use docker-compose up to start and run the
    application.</li>
    </ol>
</details>






   
<details>
 <summary>72. What is Pluggable Storage Driver architecture in Docker based containers?</summary>
  <p>Docker storage driver is by default based on a Linux file system. But Docker storage driver also has provision to plug in any other storage driver that canbe used for our environment.</p>
  <p> In Pluggable Storage Driver architecture, we can use multiple kinds of file systems in our Docker Container. In Docker info command we can see the Storage Driver that is set on a Docker daemon.</p>
  <p> We can even plug in shared storage systems with the Pluggable Storage Driver architecture.</p>  
</details>

<details>
 <summary>73. What are the main security concerns with Docker based containers?</summary>
  <p>Docker based containers have folowing security concerns:</p>
    <ol>
        <li><b>Kernel Sharing</b>: In a container-based system, multiple containers share same Kernel. If one container causes Kernel to go down, it wil take down al the containers. In a virtual machine environment we do not have this issue.</li>
        <li><b> Container Leakage </b> : If a malicious user gains access to one container, it can try to access the other containers on the same host. If a container has security vulnerabilities it can alow the user to access other containers on same host machine.</li>
        <li><b>Denial of Service</b>: If one container occupies the resources of a Kernel then other containers wil starve for resources. It can create a Denial of Service attack like situation</li>
        <li><b>Tampered Images </b>: Sometimes a container image can be tampered. This can lead to further security concerns. An attacker can try to run a tampered image to exploit the vulnerabilities in host machines and other containers.</li>
        <li><b>Secret Sharing</b> : Generaly one container can access other services. To access a service it requires a Key or Secret. A malicious user can gain access to this secret. Since multiple containers share the secret, it may lead to furthersecurity concerns</li>
    </ol>
</details>

<details>
 <sumary>74. How can we check the status of a Container in Docker?  </summary>
  <p>We can use docker ps –a command to get the list of al the containers in Docker. This command also returns the status of these containers.</p>
  </details>

<details>
 <summary>75. What are the main benefits of using Docker?</summary>
  <p>Docker is a very powerful tool. Some ofthe main benefits of using Docker are as folows:</p>
    <ol>
        <li><b>Utilize Developer Skills</b> : With Docker we maximize the use of Developer skils. With Docker there is less need of build or release engineers. Same Developer can create software and wrap it in one single file.</li>
        <li><b>Standard Application Image</b> :Docker based system alows usto bundle the application software and Operating system files in a single Application Image that canbe deployed independently.</li>
        <li><b>Uniform deployment</b>: With Docker we can create one package of our software and deploy it on diferent platforms seamlessly</li>
    </ol>
</details>

<details>
 <sumary>76. How does Docker simplify Software Development process?</summary>
<p>Prior to Docker, Developers would develop software and pass it to QA for testing and then it is sent to Build & Release team for deployment.</p>
<p>In Docker workflow, Developer builds an Image after developing and testing the software. This Image is shipped to Registry. From Registry it is available for deployment to any system. The development process is simpler since steps for QA and Deployment etc take place before the Image is built. So Developer gets the feedback early.</p>
</details>

<details>
 <sumary>77. What is the basic architecture behind Docker?</summary>
<p>Docker is built on client server model. Docker server is used to run the images. We use Docker client to communicate with Docker server.</p>
<p>Clientstel Dockerserver via commands what to do.</p>
<p>Additionaly there is a Registry that stores Docker Images. Docker Server can directly contact Registry to download images.</p>
</details>

<details>
 <sumary>78. What are the popular tasks that you can do with Docker Command line tool?</summary>
<p>Docker Command Line (DCL) tool is implemented in Go language. It can compile and run on most of the common operating systems. Some of the tasks that  we cando with Docker Command Line tool are as folows:</p>
 <ol>
<li> We can download images from Registry with DCL.</li>
<li> We can start, stop or terminate a container on a Docker server by DCL.</li>
<li> We can retrieve Docker Logs via DCL.</li>
<li>We can build a Container Image with DCL.</li>
</ol>
</details>

<details>
 <sumary>79. What type of applications- Stateless or Stateful are more suitable for Docker Container?</summary>
<p>It is preferable to create Stateless application for Docker Container. We can create a container out of our application and take out the configurable state parameters from application. Now we can run same container in Production as wel as QA environments with diferent parameters. This helps in re using the same Image in diferent scenarios. Also a stateless application is much easier to scale with Docker Containers than a stateful application.</p>
</details>

<details>
 <sumary>80. How can Docker run on different Linux distributions?</sumary>
<p>Docker directly works with Linux kernel level libraries. In every Linux distribution, the Kernel is same. Docker containers share same kernel as the host kernel.</p>
<p>Since al the distributions share the same Kernel, the container can run on any of these distributions.</p>
</details>

<details>
 <sumary>81. Why do we use Docker on top of a virtual machine?</sumary>
<p>Generaly we use Docker on top of a virtual machine to ensure isolation of the application. On a virtual machine we can get the advantage of security provided by hypervisor. We can implement dif erent security levels on a virtual machine. And Docker can make use of this to run the application at diferent security levels.</p>
</details>

<details>
 <sumary>82. How can Docker container share resources?</sumary>
<p>We can run multiple Docker containers on same host. These containers can share Kernel resources. Each container runs on its own Operating System and it has its ownuser-space and libraries.</p>
<p>So in a way Docker container does not share resources within its own namespace. But the resources that are not in isolated namespace are shared between containers. These are the Kernel resources of host machine that have just one copy.</p>
<p>So in the back-end there is same set of resources that Docker Containers share.</p>
</details>

<details>
 <sumary>83. What is the difference between Add and Copy command in a Docker file?</sumary>
<p>Both Add and Copy commands of Docker file can copy new files from a source locationto a destination in Container’s file path. They behave almost same.</p>
<p>The main diference between these two is that Add command can also read the files from a URL.</p>
<p>As per Docker documentation, Copy command is preferable. Since Copy only supports copying local files to a Container, it is preferred over Add command.</p>
</details>

<details>
 <sumary>84. What is Docker Entry point?</sumary>
<p>We use Docker Entry point to set the starting point for a command in a Docker Image.</p>
<p>We can use the entry point as a command for running an Image in the container.</p>
<p>E.g. We can define folowing entry point in docker file and run it as folowing command:</p>
<p>ENTRYPOINT[“mycmd”]</p>
<p>% docker runmycmd</p>
</details>

<details>
 <sumary>85. What is ONBUILD command in Docker?</sumary>
<p>We use ONBUILD command in Docker to run the instructions that have to execute after the completion of current Docker  file build.</p>
<p>It is used to build a hierarchy of images that have to be build after the parent image is built.</p>
<p>A Docker build wil execute first ONBUILD command and then it wil execute any other command in Child Dockerfile.</p>
</details>

<details>
 <sumary>86. What is Build cache in Docker?</sumary>
<p>When we build an Image, Docker wil process each line in Docker file. It wil execute the commands on each line in the order that is mentioned in the file.<p>
<p>But at each line, before running any command, Docker wil check if there is already an existing image in its cache that can be reused rather than creating a new image.<p>
<p>This method ofusing cache inDocker is caled Build cache in Docker.<p>
<p>We can also specify the option –no-cache=true to let Docker know that we do not want to use cache for Images. With this option, Docker will create al new images<p>
</details>

<details>
 <sumary>87. What are the most common instructions in Dockerfile?</sumary>
<p>Some ofthe common instructions in Docker file are as folows:</p>
 <li><b>FROM</b>: We use FROM to set the base image for subsequent instructions. In every valid Docker file, FROM is the first instruction.</li>
<li><b>LABEL</b>: We use LABEL to organize our images as per project, module, licensing etc. We can also use LABEL to help in automation. In LABEL we specify a key value pair that can be later used for programmaticaly handling the Dockerfile.</li>
<li><b>RUN</b>: We use RUN command to execute any instructions in a new layer on top of the current image. With each RUN command we add something on top ofthe image and use it in subsequent steps in Dockerfile.</li>
<li><b>CMD</b>: We use CMD command to provide default values of an executing container. In a Dockerfile, if we include multiple CMD commands, then only the last instruction is used.</li>
</details>

<details>
 <sumary>88. What is the purpose of EXPOSE command in Dockerfile?</sumary>
<p>We use EXPOSE command to in form Docker that Container wil listen on a specific network port during runtime.</p>
<p>But these ports on Container may not be accessible to the host. We can use –p to publish a range of ports from Container.</p>
</details>

<details>
 <sumary>89. What are the different kinds of namespaces available in a Container?</sumary>
<p>In a Container we have an isolated environment with namespace for each resource that a kernel provides. There are mainly six types of namespaces in a Container.</p>
<li><b>UTS Namespace</b>: UTS stands for Unix Timesharing System. In UTS namespace every container gets its own hostname and domain name.</li>
<li><b>Mount Namespace </b>: This namespace provides its own file system with in a container. With this namespace we get root like / in the file system on which rest of the file structure is based.</li>
<li><b>PID Namespace</b>: This namespace contains al the processes that run within a Container. We can run ps command to see the processes that are running with in a Docker container.</li>
<li><b>IPC Namespace</b>: IPC stands for Inter Process Communication. This namespace covers shared memory, semaphores, named pipes etc resourcesthat are shared by processes. The itemsin this namespace do not cross the container boundary.</li>
</details>



<details>
 <summary>108. Why do we use API in cloud computing environment? </summary>
  <p>Application Programming Interfaces (API) is used in cloud computing environment for accessing many services. APIs are very easy to use. They 
provide a quick option to create different set of applications in cloud environment.</p>
<p>An API provides a simple interface that can be used in multiple scenarios. </p>
<p> There are different types of clients for cloud computing APIs. It is easier to serve different needs of multiple clients with APIs in cloud computing environment.
 </p>
</details>


<details>
 <summary>109. What are the different areas of Security Management in cloud?  </summary>
  <p>Different areas of Security management in cloud are as folows:</p>
    <ol>
        <li><b>I. Identity Management : </b>This aspect creates dif erent level of users, roles and their credentials to access the services in cloud. </li>
        <li><b>II. Access Control : </b>In this area, we create multiple levels of permissions and access areas that can be given to a user or role for 
accessing a service in cloud environment. </li>
        <li><b>III. Authentication :</b> In this area, we check the credentials of a user and confirm that it is the correct user. Generaly this is done by 
user password and multi-factor authentication like-verification by a one-time use code on cel phone. </li>
        <li><b>IV. Authorization :</b> In this aspect, we check for the permissions that are given to a user or role. If a user is authorized to access a 
service, they are alowed to use it in the cloud environment.</li>
    </ol>
</details>


<details>
 <summary>110. What are the main cost factors of cloud based data center? </summary>
  <p>Costs in a Cloud based data center are different from a traditional data center. Main cost factors of cloud based data center are as follows: </p>
    <ol>
        <li><b>I. Labor cost :</b> We need skilled staff that can work with the cloud-based datacenter that we have selected for our operation. Since 
cloud is not a very old technology, it may get difficult to get the right skil people for handling cloud based datacenter. </li>
        <li><b>II. Power cost :</b> In some cloud operations, power costs are borne by the client. Since it is a variable cost, it can increase with the 
increase in scale and usage. </li>
        <li><b>III. Computing cost : </b>The biggest cost in Cloud environment is the cost that we pay to Cloud provider for giving us computing 
resources. This cost is much higher compared to the labor or power costs. </li>
    </ol>
</details>

<details>
 <summary>111. How can we measure the cloud-based services? </summary>
  <p>
In a cloud-computing environment we pay for the services that we use. So main criteria to measure a cloud based service its usage.  </p>
   <p>For computing resource we measure by usage in terms of time and the power of computing resource.  </p>
<p>For a storage resource we measure by usage in terms of bytes (giga bytes) and bandwidth used in data transfer. </p>

<p> Another important aspect of measuring a cloud service is its availability. A cloud provider has to specify the service level agreement (SLA) for the 
time for which service wil be available in cloud. </p>
    
</details>

<details>
 <summary> 112. How a traditional datacenter is different from a cloud environment?</summary>
  <p>In a traditional datacenter the cost of increasing the scale of computing environment is much higher than a Cloud computing environment. Also in a 
traditional data center, there are not much benefits of scaling down the operation when demand decreases. Since most of the expenditure is in 
capital spent of buying servers etc., scaling down just saves power cost, which is very less compared to other fixed costs.   </p>
   <p> 
Also in a Cloud environment there is no need to higher a large number of operations staf to maintain the datacenter. Cloud provider takes care of 
maintaining and upgrading the resources in Cloud environment. </p>
<p>With a traditional datacenter, people cost is very high since we have to hire a large number of technical operation people for in-house datacenter. </p>
    
</details>

<details>
 <summary> 113. How will you optimize availability of your application in a Cloud environment? </summary>
  <p> 
In a Cloud environment, it is important to optimize the availability of an application by implementing disaster recovery strategy. For disaster 
recovery we create a backup application in another location of cloud environment. In case of complete failure at a data center we use the disaster 
recovery site to run the application.   </p>
   <p> 
Another aspect of cloud environment is that servers often fail or go down. In such a scenario it is important to implement the application in such a 
way that we just kil the slow server and restart another server to handle the traffic seamlessly. </p>

</details>

<details>
 <summary>114. What are the requirements for implementing IaaS strategy in Cloud?  </summary>
  <p>Main requirements to implement IAAS are as folows:  </p>
    <ol>
        <li><b>I. Operating System(OS):</b> We need an OS to support hypervisor in IaaS. We can use open source OS like Linux for this purpose.  </li>
        <li><b>II. Networking :</b> We have to define and implement networking topology for IaaS implementation. We can use public or private 
network for this. </li>
        <li><b>III. Cloud Model :</b> We have to select the right cloud model for implementing IaaS strategy. It can be SaaS, PaaS or CaaS. </li>
    </ol>
</details>


<details>
 <summary>115. What is the scenario in which public cloud is preferred over private cloud?   </summary>
  <p> In a startup mode often we want to test our idea. In such a scenario it makes sense to setup application in public cloud. It is much faster and 
cheaper to use public cloud over private cloud.  </p>
  <p> Remember security is a major concern in public cloud. But with time and changes in technology, even public cloud is very secure.  </p>
    
</details>



<details>
 <summary>116. Do you think Cloud Computing is a software application or a hardware service? </summary>
  <p> Cloud Computing is neither a software application nor a hardware service. Cloud computing is a systemarchitecture that can be used to implement 
software as wel as hardware strategy of an organization.  </p>
  <p>Cloud Computing is a highly scalable, highly available and cost ef ective solution for software and hardware needs of an application. </p>
  <p>Cloud Computing provides great ease of use in running the software in cloud environment. It is also very fast to implement compared with any 
other traditional strategy.</p>
    
</details>

<details>
 <summary>117. Why companies now prefer Cloud Computing architecture over Client Server Architecture? </summary>
  <p>  
In Client Server architecture there is one to one communication between client and server. Server is often at in-house datacenter and client can 
access same server from anywhere. If client is at a remote location, the communication can have high latency.  </p>
  <p>In Cloud Computing there can be multiple servers in the cloud. There wil be a Cloud controler that directs the requests to right server node. In 
such a scenario clients can access cloud-based service from any location and they can be directed to the one nearest to them. </p>
  <p>Another reason for Cloud computing architecture is high availability. Since there are multiple servers behind the cloud, even if one server is down, 
another server can serve the clients seamlessly.</p>
    
</details>

<details>
 <summary>118. What are the main characteristics of Cloud Computing architecture?  </summary>
  <p>Main characteristics of Cloud Computing architecture are as folows:  </p>
  <ol>
        <li><b>I. Elasticity : </b>In Cloud Computing system is highly elastic in the sense that it can easily adapt itself to increase or decrease in load. 
There is no need to take urgent actions when there is surge in traf ic requests.</li>
        <li><b>II. Self-service provisioning :</b> In Cloud environment users can provision new resources on their own by just caling some APIs. 
There is no need to fill forms and order actual hardware from vendors.</li>
        <li><b>III. Automated de-provisioning :</b> In case demand/load decreases, extra resources can be automatically shut down in Cloud 
computing environment.</li>
        <li><b>IV. Standard Interface :</b> There are standard interfaces to start, stop, suspend or remove an instance in Cloud environment. Most of 
the services are accessible via public and standard APIs in Cloud computing. </li>
        <li><b>V. Usage based Billing : </b>In a Cloud environment, users are charged for their usage of resources. They can forecast their bil and 
costs based on the growth they are expecting in their load. </li>
    </ol>
</details>



<details>
 <summary>119. How databases in Cloud computing are different from traditional databases? </summary>
  <p> In a Cloud environment, companies often use dif erent kind of data to store. There are data like email, images, video, pdf, graph etc. in a Cloud 
environment. To store this data often NoSQL databases are used.   </p>
  <p>A NoSQL database like MongoDB provides storage and retrieval of data that cannot be stored ef iciently in a traditional RDBMS.  </p>
  <p> Database like Neo4J provides features to store graph data like Facebook, LinkedIn etc. in a cloud environment.</p>
<p>  
Hadoop like database help in storing Big Data based information. It can handle very large-scale information that is generated in a large-scale 
environment. </p>  

</details>


<details>
 <summary>120. What is Virtual Private Network (VPN)? </summary>
  <p>  In a Cloud environment, we can create a virtual private network (VPM) that can be solely used by only one client. This is a secure network in 
which data transfer between servers of same VPN is very secure.  </p>
  <p>By using VPN, an organization uses the public network in a private manner. It increases the privacy of an organization’s data transfer in a cloud 
environment.  </p>
  
</details>


<details>
 <summary>121. What are the main components of a VPN? </summary>
  <p>Virtual Private Network (VPN) consists of following main components:  </p>
  <ol>
        <li><b> Network Access Server (NAS): </b>A NAS server is responsible for setting up tunnels in a VPN that is accesses remotely. It 
maintains these tunnels that connect clients to VPN. </li>
        <li><b> Firewall :</b> It is the software that creates barrier between VPN and public network. It protects the VPN from malicious activity that can be done from the outside network.</li>
        <li><b>AAA Server : </b>This is an authentication and authorization server that controls the access and usage of VPN. For each request to use VPN, AAA server checks the user for correct permissions</li>
        <li><b>Encryption : </b>In a VPN, encryption algorithms protect the important private data from malicious users. </li>
    </ol>
</details>

<details>
 <summary>122. How will you secure the application data for transport in a cloud environment? </summary>
  <p> With ease of use in Cloud environment comes the important aspect of keeping data secure. Many organizations have data that is transferred from 
their traditional datacenter to Cloud datacenter.  </p>
  <p>During the transit of data it is important to keep it secure. Once of the best way to secure data is by using HTTPS protocol over Secure Socket 
Layer (SSL).</p>
<p>Another important point is to keep the data always encrypted. This protects data from being accessed by any unauthorized user during transit. </p>
</details>



<details>
 <summary>123. What are the large-scale databases available in Cloud? </summary>
  <p> 
In Cloud computing scale is not a limit. So there are very large-scale databases available from cloud providers. Some of these are:  </p>
  <ol>
        <li><b>I. Amazon DynamoDB :</b> Amazon Web Services (AWS) provides a NoSQL web service caled DynamoDB that provides highly 
available and partition tolerant database system. It has a multi-master design. It uses synchronous replication across multiple 
datacenters. We can easily integrate it with MapReduce and Elastic MapReduce of AWS. </li>
        <li><b>II. Google Bigtable :</b> This is a very large-scale high performance cloud based database option from Google. It is available on Google 
Cloud. It can be scaled to peta bytes. It is a Google proprietary implementation. In Bigtable, two arbitrary string values, row key 
and column key, and timestamp are mapped to an arbitrary byte array. In Bigtable MapReduce algorithm is used for modifying and 
generating the data. </li>
        <li><b>III. Microsoft Azure SQL Database :</b> Microsoft Azure provides cloud based SQL database that can be scaled very easily for 
increased demand. It has very good security features and it can be even used to build multi-tenant apps to service multiple 
customers in cloud. </li>
    </ol>
</details>



<details>
 <summary>124. What are the options for open source NoSQL database in a Cloud environment?</summary>
  <p> Most of the cloud-computing providers support Open Source NoSQL databases. Some of these databases are:</p>
  <ol>
        <li><b>I. Apache CouchDB :</b> It is a document based NoSQL database from Apache Open Source. It is compatible with Couch 
Replication Protocol. It can communicate in native JSON and can store binary data very wel.</li>
        <li><b>II. HBase :</b> It is a NoSQL database for use with Hadoop based software. It is also available as Open Source from Apache. It is a 
scalable and distributed Big Data database.  </li>
        <li><b>III. MongoDB :</b> It is an open source database system that of ers a flexible data model that can be used to store various kinds of data. 
It provides high performance and always-on user experience.</li>
    </ol>
</details>


<details>
 <summary>125. What are the important points to consider before selecting cloud computing? </summary>
  <p> Cloud computing is a very good option for an organization to scale and outsource its software/hardware needs. But before selecting a cloud 
provider it is important to consider following points:   </p>
  <ol>
        <li><b>I. Security : </b>One of the most important points is security of the data. We should ask the cloud provider about the options to keep 
data secure in cloud during transit and at rest. </li>
        <li><b>II. Data Integrity : </b>Another important point is to maintain the integrity of data in cloud. It is essential to keep data accurate and 
complete in cloud environment.  </li>
        <li><b>III. Data Loss :</b> In a cloud environment, there are chances of data loss. So we should know the provisions to minimize the data loss. It 
can be done by keeping backup of data in cloud. Also there should be reliable data recovery options in case of data loss.  </li>
<li><b>IV. Compliance :</b> While using a cloud environment one must be aware of the rules and regulations that have to be followed to use the 
cloud. There compliance issues with storing data of a user in an external provider’s location/servers.</li>
<li><b>V. Business Continuity :</b> In case of any disaster, it is important to create business continuity plans so that we can provide 
uninterrupted service to our end users.</li>
<li><b>VI. Availability :</b> Another important point is the availability of data and services in a cloud-computing environment. It is very important 
to provide high availability for a good customer experience.</li>
<li><b>VII. Storage Cost :</b> Since data is stored in cloud, it may be very cheap to store the data. But the real cost can come in transfer of data 
when we have to pay by bandwidth usage. So storage cost of data in cloud should also include the access cost of data transfer.</li>
<li><b>VIII. Computing Cost :</b> One of the highest costs of cloud is computing cost. It can be very high cost with the increase of scale. So 
cloud computing options should be wisely considered in conjunction with computing cost charged for them.</li>
    </ol>
</details>



<details>
 <summary>126. What is a System integrator in Cloud computing?</summary>
  <p>Often an organization does not know al the options available in a Cloud computing environment. Here comes the role of a SystemIntegrator (SI) who specializes in implementing Cloud computing environment.</p>
  <p>SI creates the strategy of cloud setup. It designs the cloud platform for the use of its client. It creates the cloud architecture for the business need of client.</p>
  <p>SI oversees the overall implementation of cloud strategy and plan. It also guides the client while choosing the right options in cloud computing platform.</p>
</details>

<details>
 <summary>127.  What is virtualization in cloud computing?</summary>
  <p>Virtualization is the core of cloud computing platform. In cloud we can create a virtual version of hardware, storage and operating system that can be used to deploy the application.</p>
  <p>A cloud provider gives options to create virtual machines in cloud that can be used by its clients. These virtual machines are much cheaper than buying a few high end computing machines.</p>
  <p>In cloud we can use multiple cheap virtual machines to implement a resilient software system that can be scaled very easily in quick time. Where as buying an actual high-end machine to scale the system is very costly and time taking.
</p>
</details>

<details>
 <summary>128.  What is Eucalyptus in a cloud environment?</summary>
  <p>Eucalyptusis an open source software to build private and hybrid cloud in Amazon Web Services(AWS)</p>
  <p>It stands for Elastic Utility Computing Architecture for Linking Your Programs to Useful Systems</p>
  <p>We can create our own data center in a private cloud by using Eucalyptus. It makes use of pooling the computing and storage resources to scale up the operations.</p>
  <p>In Eucalyptus, we create images of software applications. These images are deployed to create instances. These instances are used for computing needs</p>
  <p>A Eucalyptus instance can have both public and private ip addresses.</p>
</details>


<details>
 <summary>129. What are the main components of Eucalyptus cloud architecture?</summary>
  <p>The main components of Eucalyptus cloud architecture are as follows:</p>
    <ol>
        <li><b>Cloud Controller (CLC) </b> : : This is the controller that manages virtual resources like servers, network and storage. It is at the
        highest level in hierarchy. It is a Java program with web interface for outside world. It can do resource scheduling as well as systemaccounting. There is only one CLC per cloud. It can handle authentication, accounting, reporting and quota management in cloud.</li>
        <li><b>Walrus</b> :  This is another Java program in Eucalyptus that is equivalent to AWS S3 storage. It provides persistent storage. It also contains images, volumes and snapshots similar to AWS. There is only one Walrusin a cloud.</li>
        <li><b>Cluster Controller (CC) </b> : It is a C program that is the front end for a Eucalyptus cloud cluster. It can communicate with Storage controller and Node controller. It manages the instance execution in cloud.</li>
        <li><b> Storage Controller(SC)</b>: It is a Java programe quivalent to EBS in AWS. It can interface with Cluster Controller and Node Controller to manage persistent data via Walrus</li>
        <li><b>Node Controller (NC)</b>: It is a C program that can host a virtual machine instance. It is at the lowest level in Eucalyptus cloud. It
        downloads images from Walrus and creates an instance for computing requirements in cloud.</li>
        <li><b>VMWare Broker </b>:It is an optional component in Eucalyptus. It provides AWS compatible interface to VMWare environment.</li>
    </ol>
</details>

<details>
 <summary>130. What is Auto-scaling in Cloud computing?</summary>
  <p>Amazon Web Services (AWS) provides an important feature called Auto-scaling in the cloud. With Auto-scaling setup we can automaticaly provision and start new instances in AWS cloud without any human intervention.</p>
  <p>Auto-scaling is triggered based on load and other metrics</p>
  <p>Let say if the load reaches a threshold we can setup auto-scaling to kick in and start a new server to handle additional load.</p>  
</details>

<details>
 <summary>131. What are the benefits of Utility Computing model?</summary>
  <p>Utility computing is a cloud service model in which provider gives computing resources to users for using on need basis.</p>
  <p>Some of the main benefits of Utility computing are:</p>
    <ol>
        <li><b>Pay per use</b>:  Since a user pays for only usage, the cost of Utility computing is pay per use. We pay for the number of servers of instances that we use in cloud.</li>
        <li><b>Easy to Scale </b>:  It is easier to scale up the operations in Utility computing. There is no need to plan for time consuming and costly hardware purchase</li>
        <li><b>Maintenance </b>:In Utility computing maintenance of servers is done by cloud provider. So a user can focus on its core business. It need not spend time and resources on maintenance of servers in cloud.Utility computing is also known as On-demand computing.</li>
    </ol>
</details>

<details>
 <summary>132. What is a Hypervisor in Cloud Computing?</summary>
  <p>Hypervisor is also known as virtual machine monitor (VMM). It is a computer software/hardware that can create and run virtual machines.</p>
  <p>Hypervisor runs on a host machine. Each virtual machine is called Guest machine</p>
  <p>Hypervisor derives its name from term supervisor, which is a traditional name for the kernel of an operating system</p>
  <p>Hypervisor provides a virtual operating platformto the guest operating system. It manages the execution of guest OS.</p>  
</details>

<details>
 <summary>133. What are the different types of Hypervisor in Cloud Computing?</summary>
  <p>Hypervisors come in two main types:</p>
  <ol>
        <li><b>. Type-1, native orbare-metal hypervisors</b>: Type 1 hypervisor runs directly on the hardware of host machine. It controls the guest operating system from host machine. It is also caled bare metal hypervisor or native hypervisor.
        <p>Examples of Type-1 are:Xen, Oracle VM Server for SPARC, Oracle VM Server for x86, the CitrixXenServer, Microsoft Hyper-Vand VMware ESX/ESXi.</P></li>
        <li><b>Type-2, hosted hypervisors: </b>:  Type 2 hypervisor runs like a regular computer program on an operating system. The guest operating system runs like a process on the host machine. It creates an abstract guest operating systemif erent from the host operating system.<p>Examples of Type-2 are:VMware Workstation, VMware Player, VirtualBox, Paralels Desktop for Mac and QEMU are examples of type-2 hypervisors.</P>
        </li>
    </ol>
</details>

<details>
 <summary>134. Why Type-1 Hypervisor has better performance than Type-2 Hypervisor?</summary>
  <p>Type-1 Hypervisor has better performance than Type-2 hypervisor because Type-1 hypervisor skips the host operating system and it runs directly on host hardware. So it canutilize al the resources of host machine.</p>
  <p>In cloud computing Type-1 hypervisors are more popular since Cloud servers may need to run multiple operating system images.</p>
</details>

<details>
 <summary>135. What is CaaS?</summary>
  <p>CaaS is also known as Communication as a Service. It is available in Telecom domain. One of the examples for CaaS is Voice Over IP (VoIP).</p>
  <p>CaaS of ers business features like desktop call control, unified messaging, and fax via desktop</p>
  <p>CaaS also provides services for Call Center automation like- IVR, ACD, cal recording, multimedia routing and screen sharing.</p> 
</details>

<details>
 <summary>136. How is Cloud computing different from computing for mobile devices?</summary>
  <p>Since Mobile devices are getting connected to the Internet inlarge numbers, we often use Cloud computing for Mobile devices</p>
  <p>In mobile applications, there can be sudden increase in traffic as well as usage. Even some applications become viral very soon. This leads to very high load on application.</p>
  <p>In such a scenario, it makes sense to use Cloud Computing for mobile devices.</p>
  <p>Also mobile devices keep changing over time, it requires standard interfaces of cloud computing for handling multiple mobile devices.</p>  
</details>

<details>
 <summary>137. Why automation of deployment is very important in Cloud architecture?</summary>
  <p>One of the main reasons for selecting Cloud architecture is scalability of the system. In case of heavy load, we have to scale up the system so that there is no performance degradation.</p>
  <p>While scaling up the system we have to start new instances. To provision new instances we have to deploy our application on them.</p>
  <p>In such a scenario, if we want to save time, it makes sense to automate the deployment process. Another term for this is Auto-scaling</p>
  <p>With a fully automated deployment process we can start new instances based on automated triggers that are raised by load reaching a threshold.</p>  
</details>






<details>
 <summary>138. What are the main components in Amazon Cloud?</summary>
  <p>Amazon provides a wide range of products in Amazon Web Services for implementing Cloud computing architecture. In AWS some of the main components are as follows:</p>
    <ol>
        <li><b>Amazon EC2 </b>:  This is used for creating instances and getting computing power to run applications in AWS. </li>
        <li><b>Amazon S3 </b>:  This is a Simple Storage Service from AWS to store files and media in cloud.</li>
        <li><b>AmazonDynamoDB </b>:It is the database solution by AWS in cloud. It can store very large-scale data to meet needs of even Big Data computing.</li>
        <li><b> AmazonRoute53 </b>: This is a cloud based Domain Name System(DNS) service from AWS</li>
        <li><b>AmazonElastic LoadBalancing (ELB) </b>: This component can be used to load balance the various nodes in AWS cloud.</li>
        <li><b>AmazonCodeDeploy </b> :This service provides feature to automate the code deployment to any instance in AWS</li>
    </ol>
</details>


<details>
 <summary>139. What are main components in Google Cloud?</summary>
  <p>Google is a newer cloud alternative than Amazon. But Google provides many additional features than AWS. Some of the main components of Google Cloud are asfolows:</p>
    <ol>
        <li><b>Compute Engine </b>: This component provides computing power to Google Cloud users. </li>
        <li><b>Cloud Storage </b>: As the name suggests this is a cloud storage solution from Google for storing large files for application use or just serving over the Internet.  </li>
        <li><b>Cloud Bigtable </b>:It is a Google proprietary database from Google in Cloud. Now users can use this unique database for creating their applications.</li>
        <li><b> Cloud Load Balancing  </b>:This is a cloud-based load balancing service from Google.</li>
        <li><b> BigQuery </b>:It is a data-warehouse solution from Google in Cloud to perform data analytics of large scale.</li>
        <li><b> Cloud Machine Learning Platform </b>: It is a powerful cloud based machine learning product from Google to perform machine learning with APIs like- Job Search, Text Analysis, SpeechRecognition, Dynamic translation etc.</li>
        <li><b>  Cloud IAM </b>: This is an Identity and Access management tool from Google to help administrators run the security and authorization/authentication policies of an organization.</li>
    </ol>
</details>

<details>
 <summary>140. What are the major offerings of Microsoft Azure Cloud?</summary>
  <p>Microsoft is a relatively new entrant to Cloud computing with Azure cloud of ering. Some of the main products of Microsoft cloud are as follows:</p>
    <ol>
        <li><b> Azure Container Service </b>: This is a cloud computing service from Microsoft to run and manage Docker based containers  </li>
        <li><b> StorSimple </b>: It is a Storage solution fromMicrosoft for Azure cloud. </li>
        <li><b>AppService </b>:By usingApp Services, users can create Apps for mobile devices as well as websites.</li>
        <li><b> SQLDatabase  </b>:It is a Cloud based SQLdatabase from Microsoft.</li>
        <li><b>DocumentDB  </b>:This is a NoSQL database in cloud by Microsoft.</li>
        <li><b>Azure Bot Service </b>: We can use Azure Bot Service to create serverless bots that can be scaled up on demand</li>
         <li><b>Azure IoT Hub </b>:It is a solution for Internet of Things services in cloud by Microsoft.</li>
    </ol>
</details>

<details>
 <summary>141. What are the reasons of popularity of Cloud Computing architecture?</summary>
  <p>These days Cloud Computing is one of the most favorite architecture among organizations for their systems. Following are some of the reasons for popularity of Cloud Computing architecture:
</p>
    <ol>
        <li><b>IoT</b>:  With the Internet of Things, there are many types of machines joining the Internet and creating various types of interactions. In such a scenario, Cloud Computing serves well to provide scalable interfaces to communicate between the machines in IoT.
       </li>
        <li><b>Big Data  </b>: Another major trend in today’s computing is Big Data. With Big Data there is very large amount of user / machine data that is generated. Using in-house solution to handle Big Data is very costly and capital intensive. In Cloud Computing we can handle Big Data very easily since we do not have to worry about capital costs.  </li>
        <li><b>Mobile Devices </b>:A large number of users are going to Mobile computing. With a mobile device users can access a service from any location. To handle wide-variety of mobile devices,standard interfaces of Cloud Computing are very useful.</li>
        <li><b> ViralContent </b>:With growth of Social Media, content and media is getting viral i.e. It takes very short time to increase the traffic exponentialy on a server. In such a scenario Auto-scaling of Cloud Computing architecture can handle such spikes very easily.</li>
    </ol>
</details>

<details>
 <summary>142. What are the Machine Learning options from Google Cloud?</summary>
  <p>Google provides a very rich library of Machine Learning options in Google Cloud. Some of these API are:</p>
    <ol>
        <li><b>Google Cloud ML  </b>: This is a general purpose Machine Learning API in cloud. We can use pre-trained models or generate new models for machine learning with this option.  </li>
        <li><b>Google Cloud Jobs API  </b>: It is an API to link Job Seekers with Opportunities. It is mainly for job search based on skils, demand and location  </li>
        <li><b>Google Natural Language API </b>:This API can do text analysis of natural language content. We can use it for analyzing the content of blogs, websites, books etc.</li>
        <li><b>Google Cloud Speech API </b>: It is a Speech Recognition API from Google to handle spoken text. It can recognize more than 80 languages and their related variants. It can even transcribe the user speech into written text.</li>
        <li><b>Google Cloud Translate API  </b> :This API can translate content from one language to another language in cloud</li>
        <li><b>Google Cloud Vision API </b>: It is a powerful API for Image analysis. It can recognize faces and objects in an image. It can even categorize images in multiple relevant categories with a simple RESTAPI call</li>
    </ol>
</details>


<details>
 <summary>143. How will you optimize the Cloud Computing environment?</summary>
  <p>In a Cloud Computing environment we pay by usage. In such a scenario our usage costs are much higher. To optimize the Cloud Computing environment we have to keep abalance between our usage costs and usage.</p>
  <p>If we are paying for computing instances we can choose options like Lambda in AWS, which is a much cheaper options for computing in cloud.</p>
  <p>In case of Storage, if the data to be stored is not going to be accesses frequently we can go for Glacier option in AWS.</p>
  <p>Similarly when we pay for bandwidth usage, it makes sense to implement a caching strategy so that we use less bandwidth for the content that is accessed very frequently.</p> 
<p>It is a chalenging task for an architect in cloud to match the options available in cloud with the budget that an organization has to run its applications.</p>
<p>Optimizations like server-less computing, load balancing, and storage selection can help in keeping the Cloud computing costs low with no degradation inUser experience.
</p> 
</details>

<details>
 <summary>144.Do you think Regulations and Legal Compliance is an important aspect of Cloud Computing?</summary>
  <p>Yes,in Cloud Computing we are using resources that are owned by the Cloud provider. Due to this our data resides on the servers that can be shared by other users of Cloud.</p>
  <p>There are regulations and laws for handling user data. We have to ensure that these regulations are met while selecting and implementing a Cloud computing strategy.</p>
  <p>Similarly, if we are in a contract with a client to provide certain Service Level Agreement (SLA) performance, we have to implement the cloud solutionin such a way that there is no breach of SLA agreement due to Cloud provider’s failures.</p>
  <p>For security there are laws that have to be folowed irrespective of Cloud or Co-located Data center. This is in the interest of our end-customer as wel as for the benefit of business continuity.</p>
  <p>With Cloud computing architecture we have to do due diligence in selecting Security and Encryption options in Cloud.</p>    
</details>

<details>
 <summary><b>Unix Questions</b></summary>
 </details>
 <details>
 <summary>145.How will you remove all files in current directory? Including the files that are two levels down in a sub-directory.</summary>
  <p>In Unix we have rm command to remove files and sub-directories. With rmcommand we have –r option that stands for recursive. The –r option can delete all files in a directory recursively.</p>
  <p>It means if we our current directory structure is as follows:</p>
  <p>My_dir</p>
  <p>->Level_1_dir</p>
  <p>> Level_1_dir ->Level_2_dir</p>
  <p>-> Level_1_dir->Level_2_dir->a.txt</p>
  <p>Withrm–r * commandwe candelete the file a.txt as wel as sub-directories Level_1_dir and Level_2_dir.</p>
  <p><b>Command:</b></p>
  <p>rm– r *</p>
  <p>The asterisk (*)is a wildcard character that stands for al the files with any name.</p>

</details>

<details>
 <summary>146.What is the difference between the –v and –x options in Bash shell scripts?</summary>
  <p>In a BASH Unix shell we can specify the options –v and –x on top of a script as follows:</p>
  <p>#!/bin/bash -x –v</p>
  <p>With –x option BASH shell will echo the commands like for, select, case etc. after substituting the arguments and variables. So it will be an expanded form of the command that shows all the actions of the script. It is very useful for debugging a shel script.</p>
  <p>With –v option BASH shel will echo every command before substituting the values of arguments and variables. In –v option Unix wil print each line asit reads</p>
  <p>In –v option, If we run the script, the shel prints the entire file and then executes. If we run the script interactively, it shows each command after pressing enter</p>
</details>



<details>
 <summary>147.What is a Filter in Unix command?</summary>
  <p>In Unix there are many Filter commands like- cat, awk, grep, head, tail cut etc.</p>
  <p>A Filter is a software programthat takes an input and produces an output, and it can be used in a stream operation</p>
  <p>E.g. cut -d :-f 2 /etc/passwd | grep abc</p>
  <p>We canmixand match multiple filtersto create a complex command that can solve a problem.</p>
  <p>Awk and Sed are complex filters that provide fuly programmable features.</p>
  <p>Even Data scientists use Unix filters to get the overview of data stored inthe files.</p>
    
</details>

<details>
 <summary>148.What is Kernel in Unix operating system?</summary>
  <p>Kernel is the central core component of a Unix operating system(OS).</p>
  <p>A Kernel is the main component that can control everything with in Unix OS</p>
  <p>It is the first program that is loaded on startup of Unix OS. Once it is loaded it wil manage the rest of the startup process</p>
  <p>Kernel manages memory, scheduling as well as communication with peripherals like printers, keyboards etc.</p>
  <p>But Kernel does not directly interact with a user. For a new task, Kernel wil spawn a shel and user will work in a shel</p>
  <p>Kernel provides many system cals. A software programinteracts withKernelby usingsystemcals.</p>
  <p>Kernel has a protected memory area that cannot be overwrit tenaccidentaly by any process.</p>
    
</details>

<details>
 <summary>149.What is a Shell in Unix OS</summary>
  <p>Shel in Unix is a user interface that is used by a user to access Unix services.</p>
  <p>Generaly a Unix Shel is a command line interface (CLI) in which users enter commands by typing or uploading a file.</p>
  <p>We use a Shel to run diferent commands and programs on Unix operating system</p>
  <p>A Shel also has a command interpreter that can take our commands and send these to be executed by Unix operating system.</p>
  <p>Some ofthe popular Shels on Unix are:Korn shel, BASH, C shel etc.</p>
    
</details>

<details>
 <summary>150.What are the different shells in Unix that you know about?</summary>
  <p>Unix has many flavors of Shel. Some of these are as folows:</p>
    <ol>
        <li>Bourne shel: We use sh for Bourne shell</li>
        <li>Bourne Again shel: We use bash to run this shell </li>
        <li>Kornshel:We canuse kshto for Kornshell.</li>
        <li>Zshel:The command to use this is zsh</li>
        <li>C shel:We use cshto runC shel.</li>
        <li>EnhancedC shel:tcshisthe command for enhancedC shell</li>       
    </ol>
</details>

<details>
 <summary>151. What is the first character of the output in ls –l command ?</summary>
  <p>We use ls -l command to list the files and directories in a directory. With -l option we get long listing format</p>
  <p>In this format the first character identifies the entry type. The entry type can be one of the folowing:</p>
  <p> b Block special file</p>
  <p>c Character special file</p>
  <p>d Directory</p>
  <p>l Symbolic link</p>
  <p>s Socket link</p>
  <p>p FIFO</p>
  <p>- Regular file</p>
  <p>Ingeneralwe see d for directoryand - for a regular file.</p>
</details>
    

<details>
 <summary>152.What is the difference between Multi-tasking and Multi-user environment?</summary>
  <p>In a Multi-tasking environment,same user can submit more than one tasks and operating system wil execute the mat the same time.</p>
  <p>In a Multi-user environment, more than one user can interact with the operating systemat the same time.</p>
  <p>3. What is Command Substitution in Unix?</p>
  <p>Command substitution is a mechanism by which Shel passes the output of a command as an argument to another command. We can even use it to set a variable or use an argument list in a for loop</p>
  <p>E.g. rm`cat files_to_delete`</p>
  <p>In this example files_to_delete is a file containing the list of files to be deleted. cat command outputs this file and gives the output to rmcommand.rmcommand deletes the files.</p>
  <p>In general Command Substitution is represented by back quotes`</p>

</details>

<details>
 <summary>153.What is an Inode in Unix?</summary>
  <p>An Inode is a Data Structure in Unix that denotes a file or a directory on file system. It contains information about file like- location of file on the disk, access mode, ownership, file type etc</p>
  <p>Each Inode has a number that is used inthe index table. Unix kernel uses Inode number to access the contents of an Inode.</p>
  <p>We can use ls-i command to get the inode number of a file</p>
    
</details>


<details>
 <summary>154.What is the difference between absolute path and relative path in Unix file system?</summary>
  <p>Absolute path is the complete path of a file or directory from the root directory. In general root directory is represented by / symbol. If we are in a directory and want to knowthe absolute path, we canuse pwd command.</p>
  <p>Relative path is the path relative the current location in directory.</p>
  <p>E.g. In a directory structure /var/user/kevin/mail if we are in kevin directory then pwd command wil give absolute path as /var/user/kevin.</p>
  <p>Absolute path of mail folder is /var/user/kevin/mail. For mail folder./mail is the relative path of mail directory from kevin folder.
</p>
    
</details>


<details>
 <summary>155.What are the main responsibilities of a Unix Shell?</summary>
  <p>Some of the main responsibilities of a Unix Shel are as folows:</p>
    <ol>
        <li>Program Execution: A shel is responsible for executing the commands and script files in Unix. User can either interactively enter the commands in Command Line Interface called terminal or they can run a script file containing a program.</li>
        <li>Environment Setup: A shel can define the environment for a user. We can set many environment variables in a shel and use the value of these variables in our program.</li>
        <li>Interpreter: A shel acts as an interpreter for our scripts. It has a built in programming language that can be used to implement the logic.</li>
        <li>Pipeline: A shel also can hookup a pipeline of commands. When we run multiple commands separated by | pipe character, the shel takes the output of a command and passes it to next one inthe pipeline.</li>
        <li>I/O Redirection: Shel is also responsible for taking input from command line interface (CLI) and sending the output back to CLI. We use >, <,>> charactersforthis purpose</li>        
    </ol>
</details>

<details>
 <summary>156.What is a Shell variable?</summary>
  <p>A Unix Shel variable is an internal variable that a shel maintains. It is local to that Shell. It is not made available to the parent shell or child shell.</p>
  <p>We generalyuse lower case namesforshel variables in C shel.</p>
  <p>We can set the value of a shel variable by set command.</p>
  <p>E.g. % set max_threads=10</p>
  <p>To delete a Shel variable we can use unset command</p>
  <p>To use a Shel variable in a script we use $ sign in front of the variable name</p>
  <p>E.g. echo $max_threads</p>
    
</details>


<details>
 <summary>157.What are the important Shell variables that are initialized on starting a Shell?</summary>
  <p>There are folowing important Shel variables that are automaticaly initialized when a Shel starts:</p>
  <p>user:</p>
  <p>term:</p>
  <p>home:</p>
  <p>path:</p>
  <p>These Shel variables take values fromenvironment variables.</p>
  <p>If we change the value of these Shel variables then the corresponding environment variable value is also changed</p>
    
</details>

<details>
 <summary>158.How will you set the value of Environment variables in Unix?</summary>
  <p>We can use 'setenv' command to set the value ofenvironment variables.</p>
  <p>E.g. % setenv [Name] [value]</p>
  <p>% setenv MAX_TIME10</p>
  <p>To print the value ofenvironment variable we canuse 'printenv' command</p>
  <p>E.g. % printenvMAX_TIME</p>
  <p>If we just use print env then it lists all the environment variables and their values.</p>
  <p>To unset or delete an environment variable we use unsetenv command.</p>
  <p>E.g. % unsetenv MAX_TIME</p>
  <p>To use an environment variable in a command we use the prefix $ with the name of variable</p>
  <p>What is the special rule about Shell and Environment variable in Bourne Shell?</p>
  <p>In Bourne Shell, there is not much difference between Shell variable and Environment variable.</p>
  <p>Once we start a Bourne Shel, it getsthe value ofenvironment variables and defines a corresponding Shell variable. From that time onwards the shell only refers to Shel variable. But if a change is made to a Shel variable, then we have to explicitly export it to environment so that other shell or child processes can use it.</p>
  <p>Also for Shell variables we use set and unset commands</p>
    
</details>


<details>
 <summary>159.What is the difference between a System Call and a library function?</summary>
  <p>System calls are low-level kernel cals. These are handled by the kernel. System calls are implemented in kernel of Unix. An application hasto execute special hardware and system dependent instruction to runa Systemcal.</p>
  <p>A library function is also a low level call but it is implemented in user space. A library call is a regular function call whose code resides in a shared library.</p>
    
</details>


<details>
 <summary>160.What are the networking commands in Unix that you have used?</summary>
  <p>Some of the popular networking commands in Unix that we use are as folows:</p>
    <ol>
        <li><b>ping :</b>We use this command to test the reach ability of a host on an Internet Protocol(IP) network</li>
        <li><b>telnet :</b> This is another useful command to access another machine on the network. This is command uses Telnet protocol</li>
        <li><b>tracert :</b> This is short for Traceroute. It is a diagnostic command to display the route and transit delays of packets across Internet Protocol.</li>
        <li><b>ftp:</b>We use ftp commands to transfer files over the network. ftp uses File Transfer Protocol</li>
        <li><b>su :</b> This unix command is used to execute commands with the privileges of another user. It is also known as switch user, substitute user.</li>
        <li><b>ssh : </b>This is a secure command that is preferred over Telnet for connecting to another machine. It creates a secure channel over an unsecured network. It uses cryptographic protocolto make the communication secure.</li>        
    </ol>
</details>


<details>
 <summary>161. What is a Pipeline in Unix?</summary>
  <p>A Pipeline in Unix is a chain of commands that are connected througha stream in such a way that output of one command becomes inputfor another command.</p>
  <p>E.g. ls –l | grep “abc” | wc –l</p>
  <p>In the above example we have created pipeline ofthree commandsls, grep and wc</p>
  <p>First ls –l command is executed and gives the list of files in a directory. Then grep command searches for any line with word “abc” in it. Finaly wc –l command countsthe number of lines that are returned by grep command.</p>
  <p>In general a Pipeline is uni-directional. The data flows from left to right direction.</p>
    
</details>

<details>
 <summary>162.What is the use of tee command in Unix?</summary>
  <p>We use tee command in a shell to read the input by user (standard input) and write it to screen (standard output) as well as to a file.</p>
  <p>We can use tee command to split the output of a program so that it is visible on command line interface (CLI) as well as stored on a file for later use.</p>
  <p>Syntax is tee [-a] [-i] [file]</p>
</details>

<details>
 <summary>163.How will you count the number of lines and words in a file in Unix?</summary>
  <p>We can use wc (word count) command for counting the number of lines and words in a file. The wc command provides very good options for collecting statistics of a file. Some of these options are:</p>
   <p>l:This optio ngives line count</p>
   <p>m:This option gives character count</p>
   <p>c :This option gives byte count</p>
   <p>w:This option gives word count</p>
   <p>L:This option gives the length of the longest line</p>
   <p>In case we give more than one files as input to wc command then it gives statistics for individual files as well as the total statistics for all files.</p>
</details>

<details>
 <summary>164.What is Bash shell?</summary>
  <p>Bash stands for Bourne Again Shell. It i sfree software written to replace Bourne shell.</p>
  <p>We can see following line in shell scripts for Bash shell.</p>
  <p>#!/bin/bash</p>
  <p>In Bash we use ~/.profile at login to set environment variables.</p>
  <p>In Bash we can execute commands in batch mode or concurrent mode.</p>
  <p>Inbatchmode commands are separated by semi colon.</p>
  <p>% command1; command2</p>
  <p>In concurrent mode we separate commands by & symbol.</p>
  <p> % command1 & command2</p>
</details>



<details>
 <summary>165. How will you search for a name in Unix files?</summary>
  <p>The typical DevOps workflow in our organization is as follows:</p>
  <p>We can use grep command to search for a name or any text in a Unix file.</p>
  <p> Grep stands for Globaly search a Regular Expression and Print.</p>
  <p> Grep command can search for a text in one file as well as multiple files.</p>
  <p> We can also specify the text to be searched in regular expression pattern.</p>
  <p> % grep ^z *.txt </p>
  <p>Above command searches for lines starting with letter z in all the .txt files in current directory.</p>
</details>

<details>
 <summary>166. What are the popular options of grep command in Unix?</summary>
  <p>In Unix, grep is one of the very useful commands. It provides many useful options. Some of the popular    options are:</p>
  <p>% grep -i :This option ignores case while doing search.</p>
  <p>% grep -x :This option is used to search exact word in a file.</p>
  <p>% grep -v:We use this option to find the lines that do not have the text we are searching.</p>
  <p>% grep -A10:This option displays 10 lines after the match is found.</p>
  <p>% grep -c:We can use it to count the number of matching lines.</p>
</details>

<details>
 <summary>167. What is the difference between whoami and who am i commands in Unix?</summary>
  <p>Both the commands whoami and who am i are used to get the user information in Unix</p>
  <p>When we login as root user on the network, then both whoami and who am i commands will show the user as root.</p>
  <p>But when any other user let say john logs in remotely and runs su -root, whoamiwil show root, but who am i will show the original user john.</p>
</details>

<details>
 <summary>168. What is a Superuser in Unix?</summary>
  <p>Superuser is a special user account. It is used for Unix system administration. This user can access all files on the file system. Also Superuser can also run any command on a system.</p>
  <p>Generaly Superuser permission is given to root user.</p>
  <p>Most of the users work on their own user accounts. But when they need to run some additional commands, they can use su to switch to Superuser account.</p>
  <p>It is a best practice to not use Superuser account for regular operations.</p>
</details>

<details>
 <summary>169. How will you check the information about a process in Unix?</summary>
  <p>We can use ps command to check the status of a process in Unix. It is short for Process Status.</p>
  <p>On running ps command we get the list of processes that are executing in the Unix environment.</p>
  <p>Generaly we use ps -ef command. In this e stands for every process and f stands for full format.</p>
  <p>This command gives us id of the process. We canuse this id to kill the process.</p>
</details>

<details>
 <summary>170. What is the use of more command with cat command?</summary>
  <p>We generaly use cat command to display the contents of a file</p>
  <p>If a file is very big then the contents ofthe file wil not fit in screen, therefore screen will scroll forward and in the end we just see the last page of information from a file.</p>
  <p>With more command we can pause the scrolling of data from a file in display. If we use cat command with more then we just see the first page of a file first. On pressing enter button, more command will keep changing the page. In this way it is easier to view information in a file.</p>   
  <p>When using the cat command to display file contents, large data that does not fit on the screen would scroll of without pausing, therefore making it dificult to view. On the other hand, using the more command is more appropriate in such case because it wil display file contents one screen page at a time.</p>
</details>

<details>
 <summary>171. What are the File modes in Unix?</summary>
  <p>In Unix, there are three main permissions for a File:</p>
    <ol>
        <li>r = It means a user can read the file</li>
        <li>w = It means that a user can write to this file</li>
        <li>x = It means the a user can execute a file like a shell script</li>
    </ol>
  <p>Further there are three permissionsets.</p>
    <ol>
        <li>Owner:User who created the file</li>
        <li>Group:This applies to user of a group to which owner belongs</li>
        <li>Other:This is rest of the users in Unix system</li>
    </ol>
  <p>With the combination of these three sets permissions of file in Unix are specified.</p>
  <p>E.g. If a file has permissions -rwxr-xr-- , it means that owner has read, write, execute access. Group has read and execute access. Others have just read access. So the owner or admin has to specificaly grant access to Others to execute the file</p>
</details>

<details>
<summary>172.  We wrote a shell script in Unix but it is not doing anything. What could be the reason?</summary>
  <p>After writing a shell script we have to give it execute permission so that it can be run in Unix shell.</p>
  <p>We can use chmod command to change the permission of a file in Unix.In general we use chmod+x to give execute permission to users for executing the shell script.</p>
  <p>E.g. chmod+x abc.txt will give execute permission to users for executing the file abc.txt</p>   
  <p>With chmod command we can also specify to which user/group the permission should be granted. The options are:
  </p>
</details>

<details>
    <summary>173.</summary>
    <p> u is the owner user </p>
</details>

<details>
    <summary>174.</summary>
    <p> g is the owner group </p>
</details>

<details>
    <summary>175.</summary>
    <p> o is others </p>
</details>

<details>
    <summary>176.</summary>
    <p> a is all users </p>
</details>

<details>
<summary>177. What is the significance of 755 in chmod 755 command?</summary>
  <p>We use chmod command to change the permissions of a file in Unix. In this command we can pass the file permissions in the form of a three-digit number</p>
  <p>In this number 755, first digit 7 is the permissions given to owner, second digit 5 is the permissions of group and third digit 5 is the permissions of all others.</p>
  <p>Also the numbers 7 and 5 are made from following rules:</p>   
  <p>4 = read permission</p>
  <p>2 = write permission</p>
  <p>1 = execute permission</p>
  <p>So 7 = 4 + 2 + 1 = Read + Write + Execute permission</p>
  <p>5 = 4 + 1 = Read + Execute permission</p>
  <p>In out example 755 means, owner has read, write and execute permissions. Group and others have read and execute permissions</p>
</details>

<details>
<summary>178.  How can we run a process in background in Unix? How can we kill a process running in background?
</summary>
  <p>In Unix shell we can use symbol & to run a command in background.</p>
  <p>E.g. % ls -lrt &</p>
  <p>Once we use & option it runs the process in background and prints the process ID. We cannot down this process ID for using it in kil command.</p>   
  <p>We can also use ps -ef command to get the process ID of processes running in background.</p>
  <p>Once we know the process ID of a process we can kil it by folowing command:</p>
  <p>% kil -9 processId</p>
</details>

<details>
<summary>179.How will you create a read only file in Unix?</summary>
  <p>We can create a file with Vi editor, cat or any other command. Once the file is created we have to give read only permissions to file. To change file permission to read only we use following command:</p>
  <p>%chmod 400 filename</p>
</details>

<details>
<summary>180. How does alias work in Unix?</summary>
<p>We use alias in Unix to give a short name to a long command.</p>
<p> We can even use it to combine multiple commands and give a short convenient
name.</p>
<p>E.g. alias c=’clear’</p>
<p>Withthis alias we just need to type c for runningclear command.</p>
<p>In bashwe store aliasin .bash_profile file.</p>
<p>To get the list of al active alias in a shel we can run the alias command without any argument on command line.</p>
<p><b>
% alias
alias h='history'
alias ki='kil -9'
aliasl='last'</b>
</p>
</details>
<details>
<summary>181. How can you redirect I/O in Unix?</summary>
<p>In Unix we can redirect the output of command or operation to a file instead of command line interface (CLI). For this we sue redirection pointers.</p>
<p>These are symbols > and >>.</p>
<p>Ifwe want to write the output ofls –lrt command to a file we use folowing:</p>
<p>%ls –lrt > fileList.txt</p>
<p>Ifwe want to copy one file to another file we use folowing:</p>
<p>% catsrcFile > copyFile</p>
<p>Ifwe want to append the contents ofone file at the end of another fil ewe use folowing:<p>
<p>% catsrcFile >> appendToFile</p>

</details>
<details>
<summary>182. What are the main steps taken by a Unix Shell for processing a
command?</summary>
<p>A Unix Shell takes following main steps to process a command:<p>
<ol>
<li><b>I. Parse </b>: First step is to parse the command or set of commands given in a Command Line Interface (CLI). In this step multiple
consecutive spaces are replaced bysingle space. Multiple commandsthat are delimited bya symbol are divided intomultiple
individual actions.</li>
<li><b>II. Variable </b>: In next step Shell identifies the variables mentioned in commands. Generaly any word prefixed by $ sign is a variable.</li>
<li><b>III. CommandSubstitution</b>:In this step, Shell executesthe commandsthat are surrounded by back quotes and replaces that section
with the output from the command.</li>
<li><b>IV. Wild Card </b>: Once these steps are done, Shell replaces the Wild card characters like asterisk * with the relevant substitution.</li>
<li><b>V. Execute</b> :Finaly, Shell executes all the commands and folowsthe sequence inwhichCommands are given in CLI.</li>
</ol>
</details>
<details>
<summary>183. What is a Sticky bit in Unix?</summary>
<p>A Sticky bit is a file/directory permission feature inUnix.</p>
<p>Sometimes whenwe give write permission to another user then that user can delete the file without the owner knowing about it. </p>
<p>To preventsuchan
accidental deletion of file we use sticky bit.</p>
<p>When we mark a file/directory with a sticky bit, no user other than owner offile/directory gets the privilege to delete a file/directory.</p>
<ol>
<p><b>To set the sticky bit we use folowing command:</b></p>
<li>%chmod+tfilename</li>
<li>When we do ls for a file or directory, the entries with sticky bit are listed with letter t in the end of permissions.</li>
<p>E.g. %ls –lrt
-rwxrwxrwt 5 abc abc 4096 Jan 1 10:10 abc.txt</p>
<p><b>To remove the sticky bit we use folowing command:</b></p>
<li>%chmod –t filename</li>
</ol>
</details>
<details>
<summary>184. What are the different outputs from Kill command in Unix?</summary>
<p>Kil command in Unix can return following outputs:</p>
<ol>
<li>I. 0:It means Kill command wassuccessful</li>
<li>II. -1: When we get-1 from Kill command it shows that there was some error. In addition to -1 we get EPERMor ESRCHinoutput.</li>
<p>EPERMdenotesthatsystemdoes not permittheprocessto be killed.</p>
<p>ESRCHdenotesthat process with PI Dmentioned inKil command does not exist anymore. Or due to security restrictions we
cannot accessthat process.</p>
</ol>
</details>
<details>
<summary>185. How will you customize your environment in Unix?</summary>
<p>InUnix, almost al the popular shells provide optionsto customize the environment by using environment variables. To make these customizations
permanent we canwrite these to special files that are specific to auser in a shell.</p>
<p>Once we write our customizations to these files, we keep on getting same customization when we open a new shel with same user account.</p>
<p>The special files for storing customization information for different shels at login time are:</p>
<ol>
<li><b>I. C shel: /etc/.login or ~/.cshrc</b></li>
<li><b>II. TC shel:/etc/.loginor ~/.tshrc</b></li>
<li><b>III. Korn shel: ~etc/ksh.kshrc</b></li>
<li><b>IV. Bash:~/.bash_profile</b></li>
</ol>
</details>
<details>
<summary>186. What are the popular commands for user management in Unix?</summary>
<p>InUnixwe use folowingcommandsfor User Management:</p>
<ol>
<li><b>I. id </b>: This command gives the active user id with login and groups to which user belongs.</li>
<li><b>II. who</b> : This command gives the user that is currently logged on system. It also gives the time of login.</li>
<li><b>III. last</b>: This command showsthe previous login sto the systemin a chronological order.</li>
<li><b>IV. adduser </b>: We use this command to add a new user.</li>
<li><b>V. groupadd</b>:We use this command to add a new group in the system.</li>
<li><b>VI. usermod</b>:We user usermod command to add/remove a user to a group inUnix.</li>
</ol>
</details>
<details>
<summary>187. How will you debug a shell script in Unix?</summary>
<p>A shell script is a program that can be executed in Unix shell. </p>
<p>Sometimes a shell script does not work as intended.</p>
<p> To debug and find the problem</p>
<p>in shell script we canuse the options provided by shell to debugthe script.</p>
<p>In bash shell there are x and v options that canbe used while running a script.</p>
<p><b>% bash –xv<scriptName></b></p>
<p>With option v al the input lines are printed by shell.</p>
<p> With option x al the simple commands are printed in expanded format. /p>
<p>We can see all the
arguments passed to a command with–xoption.</p>
</details>
<details>
<summary>188. What is the difference between a Zombie and Orphan process in
Unix?</summary>
<ol>
<li>Zombie is a defunct child process in Unix thatstil has entry in processtable.</li>
<li>Sometimes a child process is terminated inUnix, but the parent process stil waits onit.</li>
<li>AZombie process is dif erent from an Orphan process. An orphan processis a child process whose parent process had died. </li>
<li>Once a process is orphanit is adopted byinit process. So efectivelyitis not anorphan.</li>
<li>Therefore if a process exits without cleaning its child processes, they do not become Zombie. Instead init process adopts these child processes.</li>
<li>Zombie processes are the onesthat are not yet adopted byinit process.</li>
</ol>
</details>
<details>
<summary>189. How will you check if a remote host is still alive?</summary>
<p>We can use one of the networking commands in Unix. It is caled ping. With ping command we can ping a remote host.</p>
<p>Ping utility sends packets in an IP network with ICMP protocol. </p>
<p>Once the packet goes fromsource to destination and comes back it records the
time.</p>
<p>We can even specify the number of packets we want to send so that we colect more statistics to confirmthe result.</p>
<p>% pingwww.google.com</P>
<p>Another optionisto use telnetto remote host to check itsstatus.</p>
</details>
<details>
<summary>190. How will you get the last executed command in Unix?</summary>
<p>We can use history command to get the list commands that were executed in Unix.</p>
</p> Since we are only interested in the last executed command we
have to use tailto get the last entry.</p>
<p><b>Exact command would be asfolows:</b><p>
<p><b>% history | tail-2</b></p>
</details>
<details>
<summary>191. What is the meaning of<b> “2>&1” </b>in a Unix shell?</summary>
<p>In Unix shell file descriptor 1 isforstandard output.</p>
<p>File description 2 isforstandard error.</p>
<p>We can use <b>“2>&1”</b> in a command so that all the errors from standard error go to standard output.</p>
<p><b>%catfile 2>&1</b></P>
</details>
<details>
<summary>192. How will you find which process is taking most CPU time in Unix?</summary>
<p>In Unix, we can use top command to list the CPU time and memory used by various processes.</P>
<p>The top command lists the process IDs and CPU</P>
time, memory etc used bytopmost processes.</P>
<p>Top command keeps refreshing the screen at a specified interval.</p>
<p> So we can see over the time which process is always appearing on the top most</p>
<p>row in the result oftop command.</p>
<p>This is the processthat is consumingmost CPUtime.</p>
</details>
<details>
<summary>193. What is the difference between Soft link and Hard link in Unix?</summary>
<p>A soft link is a pointer to a file, directory or a program located in a dif erent location.</p> 
<p>A hard link can point to a programor a file but not to a
directory.</p>
<p>Ifwe move, delete or rename a file, the soft link wil be broken. But a hard link stil remains after moving the file/program.</p>
<p>We use the command ln–s for creating a soft link. But a hard link canbe created by lncommand without –s option.</p>
</details>
<details>
<summary>194. How will you find which processes are using a file?</summary>
<p>We canuse ls of command to find the list of ProcessIDs of the processes that are accessing a file in Unix.</p>
<p>Lsofstandsfor List OpenFiles.</p>
<p>Sample command is:<b>
%lsof/var</b></p>
<p>It wil list the processes that are accessing/var directoryincurrent unixsystem.<p>
<p>We can use options –i, -n and –P for diferent uses.</p>
<p>%ls of–iwil only list IP sockets.</p>
</details>
<details>
<summary>195. What is the purpose of nohup in Unix?</summary>
<p>In Unix, nohup command can be used to run a command in background. But it is dif erent from & option to run a process in background.
Nohup stands for No Hangup. 
<p>A nohup process does not stop even ifthe Unix user that started the process has logged out from the system.</P>
<p>But the process started with option & will stop when the user thatstarted the processlogs of.</p>
</details>
<details>
<summary>196. How will you remove blank lines from a file in Unix?</summary>
<p>We can use grep command for this option. Grep command gives –v option to exclude lines that do not match a pattern.</p>
<p>In an empty line there is nothing from start to end.</P>
<p> InGrep command, ^ denotes that start ofline and $ denotes the end ofline.,</p>
<p>% grep –v‘^$’lists the lines that are empty froms tart to the end.</p>
<p>Once we get this result, we can use > operator to write the output to a new file. So exact command wil be:</p>
<p>% grep –v ‘^$’ file1.txt > file2.txt</p>
</details>
<details>
<summary>197. How will you find the remote hosts that are connecting to your
system on a specific port in Unix?</summary>
<p>We canuse netstat command forthis purpose. Netstat command lists the statistics about network connections. We cangrep forthe port inwhich
we are interested</P>.
<p><b>Exact command wil be:</b>
<b>% netstst –a | grep “port number”</b></P>
</details>
<details>
<summary>198. What is xargs in Unix?</summary>
<p>We use xargs command to build and execute commands that take input fromstandard input.</P> 
<p>It is generaly used in chaining of commands.</P>
<p>Xargs breaks the list of arguments into small sub lists that canbe handled bya command.</P>
<p>Folowingis a sample command:</P>
<p>% find /path -type f-print | xargs rm</P>
<p>The above command uses find to get the list of al files in /path directory.</p>
<p>Then xargs command passes this list to rm command so that they can be
deleted.</P>
</details>
